{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "tf.get_logger().setLevel('WARNING')\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (16, 12)\n",
    "matplotlib.rcParams['axes.grid'] = False\n",
    "\n",
    "train_size = 300000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_data = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values = df.set_index(\"Date Time\")\n",
    "display(df_values.head())\n",
    "df_values = df_values.values\n",
    "print(\"df_values created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean = df_values[:train_size].mean(axis=0)\n",
    "print(\"df_mean created\")\n",
    "data_std = df_values[:train_size].std(axis=0)\n",
    "print(\"df_std created\")\n",
    "\n",
    "normalized_df = (df_values - data_mean)/data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, target_dataset, start_index, end_index, trainig_window_size,\n",
    "                      predict_window_size, prediction_steps, single_step=False):\n",
    "  \"\"\"\n",
    "    Parameters:\n",
    "\n",
    "      train_dataset - data to be splited\n",
    "      target_dataset -  what we want to predict\n",
    "      start_index - from what index to split\n",
    "      end_index - till what indext split\n",
    "\n",
    "      trainig_window_size - training size window\n",
    "      predict_window_size - prediction size window\n",
    "      prediction_steps - how many steps should we predict\n",
    "      single_step - should we predict only single steps \n",
    "    \n",
    "    Returns:\n",
    "      train_dataset, test_dataset\n",
    "  \"\"\"\n",
    "\n",
    "  data = []\n",
    "  labels = []\n",
    "\n",
    "  print(\"train_dataset shape \", train_dataset.shape)\n",
    "  print(\"target_dataset shape \", target_dataset.shape)\n",
    "\n",
    "  # setting start index for next from first input data index\n",
    "  # if end_index is not given thern end index is last valid one\n",
    "  # in train_single case \n",
    "  #         start_index = 720\n",
    "  #         end index = 300000\n",
    "  start_index = start_index + trainig_window_size   # start_index for prediction\n",
    "  if end_index is None:\n",
    "    end_index = len(train_dataset) - predict_window_size   # end_index is max available if not passed as arg\n",
    "\n",
    "  # iteration for start/end with prediction_steps 1 ( sliding window )\n",
    "  # for i in range(720, 300000, 1):\n",
    "  for i in range(start_index, end_index):\n",
    "    indices = range(i-trainig_window_size, i, prediction_steps)\n",
    "    # input_data indicaes are from range(720 - 720, 720, 6)\n",
    "    data.append(train_dataset[indices])\n",
    "\n",
    "    if single_step:\n",
    "      labels.append(target_dataset[i+predict_window_size])\n",
    "    else:\n",
    "      labels.append(target_dataset[i:i+predict_window_size])\n",
    "\n",
    "  print(\"data shape {}\\nlabels shape {}\" .format(np.array(data).shape, np.array(labels).shape))\n",
    "  return np.array(data), np.array(labels)\n",
    "\n",
    "def create_time_steps(length):\n",
    "  return list(range(-length, 0))\n",
    "\n",
    "def multi_step_plot(history, true_future, prediction):\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  num_in = create_time_steps(len(history))\n",
    "  num_out = len(true_future)\n",
    "\n",
    "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "  plt.plot(np.arange(num_out)/prediction_steps, np.array(true_future), 'bo',\n",
    "           label='True Future')\n",
    "  if prediction.any():\n",
    "    plt.plot(np.arange(num_out)/prediction_steps, np.array(prediction), 'ro',\n",
    "             label='Predicted Future')\n",
    "  plt.legend(loc='upper left')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 720\n",
    "future_target = 72\n",
    "prediction_steps = 6\n",
    "\n",
    "print(\"TRAINING DATASET\")\n",
    "x_train, y_train = prepare_data(train_dataset=df_values, \n",
    "                                target_dataset=df_values[:, 1],    # predicting temperature\n",
    "                                start_index=0,                     # 0\n",
    "                                end_index=train_size,              # 300000\n",
    "                                trainig_window_size=past_history,  # 720\n",
    "                                predict_window_size=future_target, # 72\n",
    "                                prediction_steps=prediction_steps, # 6\n",
    "                                single_step=False)\n",
    "print(\"VALIDATION DATASET\")\n",
    "x_val, y_val = prepare_data(train_dataset=df_values,           # all data\n",
    "                            target_dataset=df_values[:, 1],    # predicting temperature\n",
    "                            start_index = train_size,          # 300000\n",
    "                            end_index=None,                    # None ( we`re going till the end )\n",
    "                            trainig_window_size=past_history, # 720\n",
    "                            predict_window_size=future_target, # 72\n",
    "                            prediction_steps=prediction_steps, # 6\n",
    "                            single_step=False)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"mirrored_strategy.num_replicas_in_sync \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 128*strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"training_1_clip/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"logdir = \", log_dir)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    def create_model():\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Conv1D(filters=150,\n",
    "                                        kernel_size=11,\n",
    "                                        strides=3,\n",
    "                                        padding='same',\n",
    "                                        input_shape=x_train.shape[-2:]))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "        model.add(tf.keras.layers.LSTM(128,\n",
    "                                       dropout=0.2,\n",
    "                                       return_sequences=True))\n",
    "        model.add(tf.keras.layers.LSTM(32, \n",
    "                                       dropout=0.2, \n",
    "                                       activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(150))\n",
    "        model.add(tf.keras.layers.Dense(72))\n",
    "        return model\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                     save_best_only=True,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(verbose=1)\n",
    "\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger('logs/training.log')\n",
    "\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mae')\n",
    "    model.summary()\n",
    "    multi_step_history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "                                          validation_data=val_dataset,\n",
    "                                          callbacks=[cp_callback, tensorboard_callback, csv_logger, reduce_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_dataset.take(3):\n",
    "  multi_step_plot(x[0], y[0], model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
