{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmWKQp_i2P_S",
        "colab_type": "text"
      },
      "source": [
        "Data is taken from : http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkrFhEiPcAfg",
        "colab_type": "code",
        "outputId": "093bda0b-d128-42bf-8430-937c74420dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8tQKm84cQ9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "fe3b92f0-62a2-436c-e829-a8870fc0af46"
      },
      "source": [
        "import tarfile\n",
        "import glob\n",
        "import os\n",
        "import nltk\n",
        "import sklearn\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "imdb_path = \"./aclImdb\"\n",
        "stored_data_dir = \"./storedData\"\n",
        "stopset = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d95t1AgVcRaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzipping tar file is present\n",
        "\n",
        "#tf = tarfile.open(\"aclImdb_v1.tar\")\n",
        "#tf.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFr1hv2Icu4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading imdb data into data/labels variables\n",
        "def load_imdb(directory):\n",
        "  data = {}\n",
        "  labels = {}\n",
        "  for dataset_type in [\"train\", \"test\"]:\n",
        "    data[dataset_type] = {}\n",
        "    labels[dataset_type] = {}\n",
        "    for outcome in [\"neg\", \"pos\"]:\n",
        "      data[dataset_type][outcome] = []\n",
        "      labels[dataset_type][outcome] = []\n",
        "      \n",
        "      actual_path = os.path.join(directory, dataset_type, outcome, '*.txt')\n",
        "      files = glob.glob(actual_path)\n",
        "      #print(\"{}/{} files {}\" .format(dataset_type, outcome, len(files)))\n",
        "\n",
        "      for f in files:\n",
        "        with open(f) as file_data:\n",
        "          data[dataset_type][outcome].append(file_data.read())\n",
        "          labels[dataset_type][outcome].append(outcome)\n",
        "  \n",
        "  trainData = data['train']['neg'] + data['train']['pos']\n",
        "  trainLabels = labels['train']['neg'] + labels['train']['pos']\n",
        "  trainData, trainLabels = sklearn.utils.shuffle(trainData, trainLabels)\n",
        "  testData = data[\"test\"]['neg'] + data[\"test\"]['pos']\n",
        "  testLabels = labels[\"test\"]['neg'] + data[\"test\"]['pos']\n",
        "  testData, testLabels = sklearn.utils.shuffle(testData, testLabels)\n",
        "\n",
        "  return  trainData, trainLabels, testData, testLabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3w2q3aF6bIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf5a3e10-e3fb-43c5-e203-0847592ed655"
      },
      "source": [
        "trainData, trainLabels, testData, testLabels = load_imdb(imdb_path)\n",
        "print(\"trainData {}  trainLabels {}\" .format(len(trainData), len(trainLabels)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainData 25000  trainLabels 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWtWeaR3hNe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f8eceef2-55d7-415b-f2d7-c5291eaf7361"
      },
      "source": [
        "# preprocessing \n",
        "# removing html tags\n",
        "# removing punctuations\n",
        "# tokenizing\n",
        "# removing stopwords\n",
        "# Lemmatization\n",
        "\n",
        "def preprocessDataImpl(input_data):\n",
        "  #print(\"input_data before: {}\\n \" .format(input_data))\n",
        "  input_data = BeautifulSoup(input_data, 'html5lib').get_text().lower()\n",
        "  input_data = re.sub(r\"[^a-zA-Z0-9]\", \" \", input_data)\n",
        "  #print(\"input_data after : {} \" .format(input_data))\n",
        "\n",
        "  tokenized_data = word_tokenize(input_data)\n",
        "  #print(\"tokenized_data   : {} \" .format(tokenized_data))\n",
        "\n",
        "  tokenized_data = [word for word in tokenized_data if word not in stopset]\n",
        "  #print(\"tokenized_data without stopwords   : {} \" .format(tokenized_data))\n",
        "\n",
        "  lemmatized_data = [WordNetLemmatizer().lemmatize(word) for word in tokenized_data];\n",
        "  #print(\"lemmatized-data  : {} \" .format(lemmatized_data))\n",
        "\n",
        "  return lemmatized_data\n",
        "\n",
        "# saving data with pickle during processing\n",
        "\n",
        "def preprocessData(trainData, testData, trainLabels, testLabels, fileName = \"preprocessed_data.pkl\"):\n",
        "    # If fileName is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if fileName is not None:\n",
        "      try:\n",
        "        with open(os.path.join(stored_data_dir, fileName), \"rb\") as f:\n",
        "          cache_data = pickle.load(f)\n",
        "        print(\"Read preprocessed data from cache file:\", fileName)\n",
        "      except:\n",
        "        pass\n",
        "    \n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        words_train = list(map(preprocessDataImpl, trainData))\n",
        "        words_test = list(map(preprocessDataImpl, testData))\n",
        "        \n",
        "        if fileName is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              trainLabels=trainLabels, testLabels=testLabels)\n",
        "            with open(os.path.join(stored_data_dir, fileName), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", fileName)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test\n",
        "\n",
        "words_train, words_test, labels_train, labels_test = preprocessData(\n",
        "        trainData, testData, trainLabels, testLabels)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmjf_MXRCw1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Bag of words features from picle files\n",
        "def extract_BoW_features(trainData, testData, vocabulary_size=5000,\n",
        "                         cache_dir=stored_data_dir, fileName=\"bow_features.pkl\"):\n",
        "    \n",
        "    cache_data = None\n",
        "    if fileName is not None:\n",
        "      try:\n",
        "        with open(os.path.join(cache_dir, fileName), \"rb\") as f:\n",
        "          cache_data = joblib.load(f)\n",
        "        print(\"Read features from cache file:\", fileName)\n",
        "      except:\n",
        "        pass\n",
        "    if cache_data is None:\n",
        "      print(trainData)\n",
        "      vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
        "                preprocessor=lambda x: x, tokenizer=lambda x: x)\n",
        "      features_train = vectorizer.fit_transform(trainData).toarray()\n",
        "      features_test = vectorizer.transform(testData).toarray()\n",
        "\n",
        "      if fileName is not None:\n",
        "        vocabulary = vectorizer.vocabulary_\n",
        "        cache_data = dict(features_train=features_train, features_test=features_test,\n",
        "                          vocabulary=vocabulary)\n",
        "        with open(os.path.join(cache_dir, fileName), \"wb\") as f:\n",
        "          joblib.dump(cache_data, f)\n",
        "        print(\"Wrote features to cache file:\", fileName)\n",
        "    else:\n",
        "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
        "          cache_data['features_test'], cache_data['vocabulary'])\n",
        "    \n",
        "    return features_train, features_test, vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa-RYh9QU1OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaO2r39esoDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naiveBayes clasification\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "features_train = normalize(features_train, axis=1)\n",
        "features_test = normalize(features_test, axis=1)\n",
        "\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(features_train, labels_train)\n",
        "print(classifier)\n",
        "\n",
        "print(\" scores :\\n train {} \\n test {} \"\n",
        "      .format(classifier.score(features_train, labels_train),\n",
        "             classifier.score(features_test, labels_test)))\n",
        "\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(features_train, labels_train)\n",
        "print(classifier)\n",
        "\n",
        "print(\" scores :\\n train {} \\n test {} \"\n",
        "      .format(classifier.score(features_train, labels_train),\n",
        "             classifier.score(features_test, labels_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}