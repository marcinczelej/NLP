{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from model import BertForQALabeling\n",
    "from preprocessing import dataPreprocessor\n",
    "from parameters import *\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import horovod.tensorflow as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvd.init()\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU') \n",
    "for gpu in gpus:\n",
    "    print(gpu)\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    print(\"gpus \", gpus)\n",
    "    print(\"local rank \",hvd.local_rank())\n",
    "    tf.config.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
    "    print(tf.config.get_visible_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"google-quest-challenge/\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "submit_df = pd.read_csv(os.path.join(data_dir, \"sample_submission.csv\"))\n",
    "stack_df = pd.read_csv(os.path.join(data_dir, \"stackexchange.csv\"))\n",
    "stack_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_df = train_df[['question_title', 'question_body', 'answer']]\n",
    "train_targets_df = train_df[[\"question_asker_intent_understanding\",\n",
    "    \"question_body_critical\",\n",
    "    \"question_conversational\",\n",
    "    \"question_expect_short_answer\",\n",
    "    \"question_fact_seeking\",\n",
    "    \"question_has_commonly_accepted_answer\",\n",
    "    \"question_interestingness_others\",\n",
    "    \"question_interestingness_self\",\n",
    "    \"question_multi_intent\",\n",
    "    \"question_not_really_a_question\",\n",
    "    \"question_opinion_seeking\",\n",
    "    \"question_type_choice\",\n",
    "    \"question_type_compare\",\n",
    "    \"question_type_consequence\",\n",
    "    \"question_type_definition\",\n",
    "    \"question_type_entity\",\n",
    "    \"question_type_instructions\",\n",
    "    \"question_type_procedure\",\n",
    "    \"question_type_reason_explanation\",\n",
    "    \"question_type_spelling\",\n",
    "    \"question_well_written\",\n",
    "    \"answer_helpful\",\n",
    "    \"answer_level_of_information\",\n",
    "    \"answer_plausible\",\n",
    "    \"answer_relevance\",\n",
    "    \"answer_satisfaction\",\n",
    "    \"answer_type_instructions\",\n",
    "    \"answer_type_procedure\",\n",
    "    \"answer_type_reason_explanation\",\n",
    "    \"answer_well_written\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_title = train_X_df['question_title'].values\n",
    "q_body = train_X_df['question_body'].values\n",
    "answer = train_X_df['answer'].values\n",
    "\n",
    "targets = train_targets_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreprocessor.logger = False\n",
    "dataPreprocessor.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_q_title = stack_df['question_title'].values\n",
    "stack_q_body = stack_df['question_body'].values\n",
    "stack_answer = stack_df['answer'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedInput = dataPreprocessor.preprocessBatch(q_body, q_title, answer, max_seq_lengths=(26,260,210,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedStack = dataPreprocessor.preprocessBatch(stack_q_body, stack_q_title, stack_answer, max_seq_lengths=(26,260,210,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessedInput.shape)\n",
    "print(targets.shape)\n",
    "\n",
    "print(preprocessedStack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_gradients(gradients,\n",
    "                          step_gradients,\n",
    "                          num_grad_accumulates) -> tf.Tensor:\n",
    "    if gradients is None:\n",
    "        gradients = [flat_gradients(g) / num_grad_accumulates for g in step_gradients]\n",
    "    else:\n",
    "        for i, g in enumerate(step_gradients):\n",
    "            gradients[i] += flat_gradients(g) / num_grad_accumulates\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# This is needed for tf.gather like operations.\n",
    "def flat_gradients(grads_or_idx_slices: tf.Tensor) -> tf.Tensor:\n",
    "    '''Convert gradients if it's tf.IndexedSlices.\n",
    "    When computing gradients for operation concerning `tf.gather`, the type of gradients \n",
    "    '''\n",
    "    if type(grads_or_idx_slices) == tf.IndexedSlices:\n",
    "        return tf.scatter_nd(\n",
    "            tf.expand_dims(grads_or_idx_slices.indices, 1),\n",
    "            grads_or_idx_slices.values,\n",
    "            grads_or_idx_slices.dense_shape\n",
    "        )\n",
    "    return grads_or_idx_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    print(\"trues = \", trues.shape)\n",
    "    print(\"preds = \", preds.shape)\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def spearman_metric(y_true, y_pred):\n",
    "    corr = [\n",
    "        spearmanr(pred_col, target_col).correlation\n",
    "        for pred_col, target_col in zip(y_pred.T, y_true.T)\n",
    "    ]\n",
    "    return corr\n",
    "\n",
    "def calculate_sper(true, preds):\n",
    "    score = 0\n",
    "    for i in range(30):\n",
    "        score += np.nan_to_num(spearmanr(true[:, i], preds[:, i]).correlation)\n",
    "    return score/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, warmup_steps, num_steps, base_lr):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "    self.num_steps = tf.cast(num_steps, tf.float32)\n",
    "    self.lr = tf.cast(base_lr, tf.float32)\n",
    "\n",
    "  def __call__(self, step):\n",
    "    def warmupPhase() : return step/tf.math.maximum(1.0, self.warmup_steps)\n",
    "    def decayPhase() : return tf.math.maximum(0.0, (self.num_steps - step))/tf.math.maximum(1.0, self.num_steps - self.warmup_steps)\n",
    "\n",
    "    multiplier = tf.cond(tf.math.less(step, self.warmup_steps), warmupPhase, decayPhase)\n",
    "    \n",
    "    return self.lr * multiplier\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "fold_nr =0\n",
    "\n",
    "for train_idx, test_idx in kf.split(preprocessedInput):\n",
    "    print(\"                FOLD \", fold_nr)\n",
    "\n",
    "    # train test indices\n",
    "    train_input = tf.gather(preprocessedInput, train_idx, axis=0)\n",
    "    train_target = tf.gather(targets, train_idx, axis=0)\n",
    "\n",
    "    test_input = tf.gather(preprocessedInput, test_idx, axis=0)\n",
    "    test_target = tf.gather(targets, test_idx, axis=0)\n",
    "\n",
    "    #train dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_input, train_target)). \\\n",
    "                             shuffle(len(train_input)//4, reshuffle_each_iteration=True). \\\n",
    "                             batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "    #test dataset\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_input, test_target)). \\\n",
    "                             shuffle(len(test_input)//4, reshuffle_each_iteration=True). \\\n",
    "                             batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "    lr_scheduler = CustomSchedule(warmup_steps=warmup_steps*2, \n",
    "                                  num_steps=targets.shape[0]//batch_size, \n",
    "                                  base_lr=lr*hvd.size())\n",
    "\n",
    "    #optimizer = tfa.optimizers.AdamW(learning_rate=lr_scheduler, weight_decay=decay)\n",
    "    optimizer = tf.optimizers.Adam(learning_rate = lr_scheduler)\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    model = BertForQALabeling.from_pretrained('bert-base-uncased', num_labels=num_labels, output_hidden_states=True)\n",
    "    model.backbone.bert.pooler._trainable=False\n",
    "    trainable = model.trainable_variables\n",
    "\n",
    "    checkpoint_dir = './checkpoints/best_best_uncased_fold_{}' .format(fold_nr)\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "    \n",
    "\n",
    "    @tf.function\n",
    "    def train_step(inputs, y_true, first_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            ids_mask, type_ids_mask, attention_mask = inputs[:, 0, :], inputs[:, 1, :], inputs[:, 2, :]\n",
    "            y_pred = model(ids_mask, \n",
    "                             attention_mask= attention_mask, \n",
    "                             token_type_ids=type_ids_mask, \n",
    "                             training=True)\n",
    "            loss = tf.reduce_sum(bce_loss(y_true, y_pred)*(1. / batch_size))\n",
    "\n",
    "        tape = hvd.DistributedGradientTape(tape)\n",
    "\n",
    "        grads = tape.gradient(loss, trainable)\n",
    "\n",
    "        return loss, grads, tf.math.sigmoid(y_pred)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(inputs, y_true):\n",
    "        ids_mask, type_ids_mask, attention_mask = inputs[:, 0, :], inputs[:, 1, :], inputs[:, 2, :]\n",
    "        y_pred = model(ids_mask, \n",
    "                      attention_mask= attention_mask, \n",
    "                      token_type_ids=type_ids_mask, \n",
    "                      training=False)\n",
    "        loss = tf.reduce_sum(bce_loss(y_true, y_pred)*(1. / batch_size))\n",
    "        \n",
    "        return loss, tf.math.sigmoid(y_pred)\n",
    "\n",
    "    last_loss = 999999\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradients = None\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_preds = []\n",
    "        test_preds = []\n",
    "        train_targets = []\n",
    "        test_targets = []\n",
    "        global_batch = 0\n",
    "        for batch_nr, (inputs, y_true) in enumerate(train_ds):\n",
    "            loss, current_gradient, y_pred = train_step(inputs, y_true, batch_nr==0)\n",
    "            train_losses.append(np.mean(loss))\n",
    "            train_preds.append(y_pred)\n",
    "            train_targets.append(y_true)\n",
    "            gradients = accumulated_gradients(gradients, current_gradient, gradient_accumulate_steps)\n",
    "\n",
    "            if (batch_nr +1)%gradient_accumulate_steps ==0:\n",
    "                #print(\"batch_nr {} gradient applying\" .format(batch_nr))\n",
    "                optimizer.apply_gradients(zip(gradients, trainable))\n",
    "                global_batch +=1\n",
    "                gradients = None\n",
    "\n",
    "                if batch_nr == 0:\n",
    "                    print(\"first batch\")\n",
    "                    hvd.broadcast_variables(trainable, root_rank=0)\n",
    "                    hvd.broadcast_variables(optimizer.variables(), root_rank=0)\n",
    "\n",
    "            if batch_nr % 100 == 0 and hvd.local_rank() == 0:\n",
    "                print('Step {} loss {}'  .format(batch_nr, loss, ))\n",
    "\n",
    "        for _, (inputs, y_true) in enumerate(test_ds):\n",
    "            loss, y_pred = test_step(inputs, y_true)\n",
    "            test_losses.append(np.mean(loss))\n",
    "            test_preds.append(y_pred)\n",
    "            test_targets.append(y_true)\n",
    "\n",
    "        test_spearmans = spearman_metric(np.vstack(test_targets), np.vstack(test_preds))\n",
    "        train_spearmans = spearman_metric(np.vstack(train_targets), np.vstack(train_preds))\n",
    "        \n",
    "        print(\"epoch {} train loss {} test loss {} test spearman metric {}%6 train spearman metric {}%6\" \\\n",
    "              .format(epoch, np.mean(train_losses), np.mean(test_losses), np.mean(test_spearmans), np.mean(train_spearmans)))\n",
    "        \n",
    "        if np.mean(test_spearmans) < last_loss:\n",
    "            if hvd.rank() == 0:\n",
    "                checkpoint.save(os.path.join(checkpoint_dir, \"best_base_uncased_best_model\"))\n",
    "                last_loss = np.mean(test_spearmans)\n",
    "                print(\"saving checkpoint... \")\n",
    "    \n",
    "    \"\"\"    \n",
    "        Pseudo labeling for given fold using stackexchange data\n",
    "        \n",
    "            1. restoring best checkpoint for given fold\n",
    "            2. predicting output values for stackexchange\n",
    "            3. saving predicted data into csv file \n",
    "    \"\"\"\n",
    "    checkpoint.restore(checkpoint_dir).assert_consumed()\n",
    "    pseudo_labeling_ds = tf.data.Dataset.from_tensor_slices((preprocessedStack)).batch(batch_size=batch_size, drop_remainder=True)\n",
    "    pseudo_predictions = []\n",
    "    for _, inputs in enumerate(pseudo_labeling_ds):\n",
    "        ids_mask, type_ids_mask, attention_mask = inputs[:, 0, :], inputs[:, 1, :], inputs[:, 2, :]\n",
    "        predicted = model(ids_mask, \n",
    "                      attention_mask= attention_mask, \n",
    "                      token_type_ids=type_ids_mask, \n",
    "                      training=False)\n",
    "        \n",
    "        predicted = tf.math.sigmoid(predicted)\n",
    "\n",
    "        pseudo_predictions.extend(predicted.numpy())\n",
    "    \n",
    "    predicted_df = stack_df.copy()\n",
    "    for idx, col in enumerate(train_targets_df.columns):\n",
    "        predicted_df[col] = pseudo_predictions[:, idx]\n",
    "    \n",
    "    predicted_df.to_csv(\n",
    "        os.path.join('./dataframes/pseudo_labeled' \"best_base_uncased_fold-{}.csv\" .format(fold_nr)), index=False)\n",
    "    \n",
    "    fold_nr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
