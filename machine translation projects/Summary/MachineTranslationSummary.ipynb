{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sys.path.insert(0, r\"../utilities/\")\n",
    "sys.path.insert(0, r\"../Seq2Seq/\")\n",
    "sys.path.insert(0, r\"../Seq2SeqAttention/\")\n",
    "sys.path.insert(0, r\"../Transformer/\")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Seq2SeqTrainer import Seq2SeqTrainer\n",
    "from Seq2SeqAttentionTrainer import Seq2SeqAttentionTrainer\n",
    "from TransformerTrainer import TransformerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlots(losses, accuracy, name):\n",
    "    train_losses, test_losses = losses \n",
    "    train_accuracyVec, test_accuracyVec = accuracy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_losses, label=\"train_loss\")\n",
    "    fig_plot.plot(test_losses, label=\"test_loss\")\n",
    "    fig_plot.legend(loc=\"upper right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"loss\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"losses_plot_\" + name +  \".png\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_accuracyVec, label=\"train_accuracy\")\n",
    "    fig_plot.plot(test_accuracyVec, label=\"test_accuracy\")\n",
    "    fig_plot.legend(loc=\"lower right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"accuracy\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"accuracy_plot.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from  ../data/fra-eng/fra.txt\n",
      "en_vocab 8373\n",
      "fr_vocab 13578\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "# reading data\n",
    "\n",
    "data = read_data(os.path.join(data_dir, \"fra-eng\"), \"fra.txt\")\n",
    "en_lines, fr_lines = list(zip(*data))\n",
    "en_lines_raw, fr_lines_raw = shuffle(en_lines, fr_lines)\n",
    "\n",
    "en_lines = en_lines_raw[:40000]\n",
    "fr_lines = fr_lines_raw[:40000]\n",
    "\n",
    "en_lines = [normalize(line) for line in en_lines]\n",
    "fr_lines = [normalize(line) for line in fr_lines]\n",
    "\n",
    "en_train, en_test, fr_train, fr_test = train_test_split(en_lines, fr_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "en_lines = en_test\n",
    "fr_lines = fr_test\n",
    "\n",
    "fr_train_in = ['<start> ' + line for line in fr_train]\n",
    "fr_train_out = [line + ' <end>' for line in fr_train]\n",
    "\n",
    "fr_test_in = ['<start> ' + line for line in fr_test]\n",
    "fr_test_out = [line + ' <end>' for line in fr_test]\n",
    "\n",
    "fr_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "input_data = [fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train]\n",
    "fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train = tokenizeInput(input_data,\n",
    "                                                                                      fr_tokenizer)\n",
    "input_data = [en_train, en_test]\n",
    "en_train, en_test = tokenizeInput(input_data, en_tokenizer)\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "print(\"en_vocab {}\\nfr_vocab {}\" .format(en_vocab_size, fr_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TEXTS\n",
      "You lied to Tom.  -  Vous avez menti à Tom.\n",
      "I'm all done.  -  J'ai tout fini.\n",
      "I wonder how that works.  -  Je me demande comment cela fonctionne.\n",
      "Thirteen percent were opposed.  -  Treize pour cent étaient contre.\n",
      "I went snorkeling in a beautiful coral reef.  -  J'ai été nager avec un tuba dans un beau récif corallien.\n",
      "I thought you were having a good time.  -  J'ai pensé que tu prenais du bon temps.\n",
      "Our school has about one thousand students.  -  Notre école compte environ mille étudiants.\n",
      "I don't know anything about cricket.  -  Je ne connais rien au cricket.\n",
      "The party's on Monday.  -  La fête a lieu lundi.\n",
      "I do want to be your friend.  -  Je veux vraiment être votre ami.\n"
     ]
    }
   ],
   "source": [
    "prediction_idx = np.random.randint(low=40000, high=len(en_lines_raw), size=10)\n",
    "print(\"TEST_TEXTS\")\n",
    "test_text = [(en_lines_raw[idx], fr_lines_raw[idx]) for idx in prediction_idx]\n",
    "for (en,fr) in test_text:\n",
    "    print(en, \" - \", fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab 8373\n",
      "fr_vocab 13578\n",
      "Number of devices: 4\n",
      "creating dataset...\n",
      "dataset created\n",
      "input :  She urged him to study harder .\n",
      "output:  Elle l exhorta a travailler plus fort .\n",
      "training from scratch\n",
      "starting training with 20 epochs with prediction each 1 epoch\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=1)\n",
    "losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "makePlots(losses, accuracy, \"Seq2Seq\")\n",
    "for (en_text, fr_text) in test_text:\n",
    "    trainer.predict(en_text, fr_text)\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqAttentionTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=1)\n",
    "losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS, \"concat\")\n",
    "makePlots(losses, accuracy, \"Seq2SeqAttention\")\n",
    "for (en_text, fr_text) in test_text:\n",
    "    trainer.predict(en_text, fr_text, print_prediction=True)\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "num_layers = 4 # 6\n",
    "d_model = 256 # 512\n",
    "dff = 512  # 2048\n",
    "num_heads = 8\n",
    "trainer = TransformerTrainer(BATCH_SIZE, num_layers, d_model, dff, num_heads, predict_every=1)\n",
    "losses, accuracy= trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "makePlots(losses, accuracy, \"Transformer\")\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
