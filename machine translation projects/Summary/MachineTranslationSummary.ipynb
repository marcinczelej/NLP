{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sys.path.insert(0, r\"../utilities/\")\n",
    "sys.path.insert(0, r\"../Seq2Seq/\")\n",
    "sys.path.insert(0, r\"../Seq2SeqAttention/\")\n",
    "sys.path.insert(0, r\"../Transformer/\")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Seq2SeqTrainer import Seq2SeqTrainer\n",
    "from Seq2SeqAttentionTrainer import Seq2SeqAttentionTrainer\n",
    "from TransformerTrainer import TransformerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePlots(losses, accuracy, name):\n",
    "    train_losses, test_losses = losses \n",
    "    train_accuracyVec, test_accuracyVec = accuracy\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_losses, label=\"train_loss\")\n",
    "    fig_plot.plot(test_losses, label=\"test_loss\")\n",
    "    fig_plot.legend(loc=\"upper right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"loss\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"losses_plot_\" + name +  \".png\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_accuracyVec, label=\"train_accuracy\")\n",
    "    fig_plot.plot(test_accuracyVec, label=\"test_accuracy\")\n",
    "    fig_plot.legend(loc=\"lower right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"accuracy\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"accuracy_plot.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from  ../data/fra-eng/fra.txt\n",
      "en_vocab 8467\n",
      "fr_vocab 13610\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "# reading data\n",
    "\n",
    "data = read_data(os.path.join(data_dir, \"fra-eng\"), \"fra.txt\")\n",
    "en_lines, fr_lines = list(zip(*data))\n",
    "en_lines_raw, fr_lines_raw = shuffle(en_lines, fr_lines)\n",
    "\n",
    "en_lines = en_lines_raw[:40000]\n",
    "fr_lines = fr_lines_raw[:40000]\n",
    "\n",
    "en_lines = [normalize(line) for line in en_lines]\n",
    "fr_lines = [normalize(line) for line in fr_lines]\n",
    "\n",
    "en_train, en_test, fr_train, fr_test = train_test_split(en_lines, fr_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "en_lines = en_test\n",
    "fr_lines = fr_test\n",
    "\n",
    "fr_train_in = ['<start> ' + line for line in fr_train]\n",
    "fr_train_out = [line + ' <end>' for line in fr_train]\n",
    "\n",
    "fr_test_in = ['<start> ' + line for line in fr_test]\n",
    "fr_test_out = [line + ' <end>' for line in fr_test]\n",
    "\n",
    "fr_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "input_data = [fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train]\n",
    "fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train = tokenizeInput(input_data,\n",
    "                                                                                      fr_tokenizer)\n",
    "input_data = [en_train, en_test]\n",
    "en_train, en_test = tokenizeInput(input_data, en_tokenizer)\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "print(\"en_vocab {}\\nfr_vocab {}\" .format(en_vocab_size, fr_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TEXTS\n",
      "Could you please fix this?  -  Pourriez-vous réparer ceci, s'il vous plaît ?\n",
      "Please refrain from smoking here.  -  Abstiens-toi de fumer ici, je te prie.\n",
      "That's not the answer.  -  Ce n'est pas la réponse.\n",
      "Hey, Tom, can I ask you something?  -  Hé, Tom, je peux te demander un truc ?\n",
      "It's garbage.  -  Ce sont des détritus.\n",
      "What would I do if they really came?  -  Que ferais-je si elles venaient vraiment ?\n",
      "I was happy to see him.  -  J'étais content de le voir.\n",
      "How lovely!  -  Comme c'est charmant !\n",
      "Are you saying you can't fix it?  -  Es-tu en train de dire que tu ne peux pas le réparer ?\n",
      "He has not come yet. Something may have happened to him.  -  Il n'est pas encore là. Il lui est peut-être arrivé quelque chose.\n"
     ]
    }
   ],
   "source": [
    "prediction_idx = np.random.randint(low=40000, high=len(en_lines_raw), size=10)\n",
    "print(\"TEST_TEXTS\")\n",
    "test_text = [(en_lines_raw[idx], fr_lines_raw[idx]) for idx in prediction_idx]\n",
    "for (en,fr) in test_text:\n",
    "    print(en, \" - \", fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Seq2SeqPredictions():\n",
    "    trainer = Seq2SeqTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=10)\n",
    "    losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "    makePlots(losses, accuracy, \"Seq2Seq\")\n",
    "    for (en_text, fr_text) in test_text:\n",
    "        trainer.predict(en_text, fr_text)\n",
    "\n",
    "def Seq2SeqAttentionPredictions():\n",
    "    trainer = Seq2SeqAttentionTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=10)\n",
    "    losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS, \"concat\")\n",
    "    makePlots(losses, accuracy, \"Seq2SeqAttention\")\n",
    "    for (en_text, fr_text) in test_text:\n",
    "        trainer.predict(en_text, fr_text, print_prediction=True)\n",
    "\n",
    "def TransformerPredictions():\n",
    "    BATCH_SIZE = 64\n",
    "    num_layers = 4 # 6\n",
    "    d_model = 256 # 512\n",
    "    dff = 512  # 2048\n",
    "    num_heads = 8\n",
    "    trainer = TransformerTrainer(BATCH_SIZE, num_layers, d_model, dff, num_heads, predict_every=10)\n",
    "    losses, accuracy= trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "    makePlots(losses, accuracy, \"Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab 8467\n",
      "fr_vocab 13610\n",
      "Number of devices: 4\n",
      "creating dataset...\n",
      "dataset created\n",
      "input :  I want to know what you see right now .\n",
      "output:  Je veux savoir ce que tu vois a l instant .\n",
      "training from scratch\n",
      "starting training with 1 epochs with prediction each 10 epoch\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1 training Loss 50.6450 Accuracy 0.0464  test Loss 43.6379 Accuracy 0.0744\n",
      "Saving checkpoint for epoch 0: ./checkpoints/Seq2Seq/ckpt-1\n",
      "--------------PREDICTION--------------\n",
      "  English   :  I want to know what you see right now .\n",
      "  Predicted :  je ne ne pas pas pas pas pas .\n",
      "  Correct   :  Je veux savoir ce que tu vois a l instant .\n",
      "------------END PREDICTION------------\n",
      "--------------PREDICTION--------------\n",
      "  English   :  Could you please fix this?\n",
      "  Predicted :  je est .\n",
      "  Correct   :  Pourriez-vous réparer ceci, s'il vous plaît ?\n",
      "------------END PREDICTION------------\n",
      "--------------PREDICTION--------------\n",
      "  English   :  Please refrain from smoking here.\n",
      "  Predicted :  je ne a .\n",
      "  Correct   :  Abstiens-toi de fumer ici, je te prie.\n",
      "------------END PREDICTION------------\n",
      "--------------PREDICTION--------------\n",
      "  English   :  That's not the answer.\n",
      "  Predicted :  est .\n",
      "  Correct   :  Ce n'est pas la réponse.\n",
      "------------END PREDICTION------------\n",
      "--------------PREDICTION--------------\n",
      "  English   :  Hey, Tom, can I ask you something?\n",
      "  Predicted :  je est .\n",
      "  Correct   :  Hé, Tom, je peux te demander un truc ?\n",
      "------------END PREDICTION------------\n"
     ]
    }
   ],
   "source": [
    "p = Process(target=Seq2SeqPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab 8467\n",
      "fr_vocab 13610\n",
      "Number of devices: 4\n",
      "creating dataset...\n",
      "dataset created\n",
      "input :  I want to know what you see right now .\n",
      "output:  Je veux savoir ce que tu vois a l instant .\n",
      "training from scratch\n",
      "starting training with 1 epochs with prediction each 10 epoch\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1 training Loss 1.4241 Accuracy 0.0323  test Loss 1.5293 Accuracy 0.0649\n",
      "Saving checkpoint for epoch 0: ./checkpoints/Seq2SeqAttention/ckpt-1\n",
      "----------------------------PREDICTION----------------------------\n",
      "       En sentence I want to know what you see right now . \n",
      "       Predicted:  je je je je je je je je je je je je je je je je je je je . \n",
      "       Should be:  Je veux savoir ce que tu vois a l instant . \n",
      "--------------------------END PREDICTION--------------------------\n",
      "Saving checkpoint for end at ./checkpoints/Seq2SeqAttention/ckpt-2\n",
      "----------------------------PREDICTION----------------------------\n",
      "       En sentence Could you please fix this? \n",
      "       Predicted:  je je je est . \n",
      "       Should be:  Pourriez-vous réparer ceci, s'il vous plaît ? \n",
      "--------------------------END PREDICTION--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-436f1dfd4c3d>\", line 13, in Seq2SeqAttentionPredictions\n",
      "    trainer.predict(en_text, fr_text, print_prediction=True)\n",
      "  File \"../Seq2SeqAttention/Seq2SeqAttentionTrainer.py\", line 34, in predict\n",
      "    word = self.fr_tokenizer.index_word[decoder_in.numpy()[0][0]]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 933, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-8d71ef87584f>\", line 3, in <module>\n",
      "    p.join()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 50, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/_api/v2/compat/__init__.py\", line 23, in <module>\n",
      "    from . import v1\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/_api/v2/compat/v1/__init__.py\", line 64, in <module>\n",
      "    from . import raw_ops\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 487, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "p = Process(target=Seq2SeqAttentionPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Process(target=TransformerPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
