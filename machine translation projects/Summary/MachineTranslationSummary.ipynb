{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import Process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sys.path.insert(0, r\"../utilities/\")\n",
    "sys.path.insert(0, r\"../Seq2Seq/\")\n",
    "sys.path.insert(0, r\"../Seq2SeqAttention/\")\n",
    "sys.path.insert(0, r\"../Transformer/\")\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Seq2SeqTrainer import Seq2SeqTrainer\n",
    "from Seq2SeqAttentionTrainer import Seq2SeqAttentionTrainer\n",
    "from TransformerTrainer import TransformerTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seqloss = []\n",
    "Seq2SeqAttentionloss = []\n",
    "Transformerloss = []\n",
    "\n",
    "def makePlots(losses, accuracy, name):\n",
    "    train_losses, test_losses = losses \n",
    "    train_accuracyVec, test_accuracyVec = accuracy\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_losses, label=\"train_loss\")\n",
    "    fig_plot.plot(test_losses, label=\"test_loss\")\n",
    "    fig_plot.legend(loc=\"upper right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"loss\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"losses_plot_\" + name +  \".png\")\n",
    "    fig.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(train_accuracyVec, label=\"train_accuracy\")\n",
    "    fig_plot.plot(test_accuracyVec, label=\"test_accuracy\")\n",
    "    fig_plot.legend(loc=\"lower right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"accuracy\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"accuracy_plot.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from  ../data/fra-eng/fra.txt\n",
      "en_vocab 8331\n",
      "fr_vocab 13576\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "# reading data\n",
    "\n",
    "#en_lines, fr_lines = read_data_files(data_dir, (\"small_vocab_en\", \"small_vocab_fr\"))\n",
    "\n",
    "data = read_data(os.path.join(data_dir, \"fra-eng\"), \"fra.txt\")\n",
    "en_lines, fr_lines = list(zip(*data))\n",
    "en_lines_raw, fr_lines_raw = shuffle(en_lines, fr_lines)\n",
    "\n",
    "en_lines = en_lines_raw[:40000]\n",
    "fr_lines = fr_lines_raw[:40000]\n",
    "\n",
    "en_lines = [normalize(line) for line in en_lines]\n",
    "fr_lines = [normalize(line) for line in fr_lines]\n",
    "\n",
    "en_train, en_test, fr_train, fr_test = train_test_split(en_lines, fr_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "en_lines = en_test\n",
    "fr_lines = fr_test\n",
    "\n",
    "fr_train_in = ['<start> ' + line for line in fr_train]\n",
    "fr_train_out = [line + ' <end>' for line in fr_train]\n",
    "\n",
    "fr_test_in = ['<start> ' + line for line in fr_test]\n",
    "fr_test_out = [line + ' <end>' for line in fr_test]\n",
    "\n",
    "fr_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "input_data = [fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train]\n",
    "fr_train_in, fr_train_out, fr_test_in, fr_test_out, fr_test, fr_train = tokenizeInput(input_data,\n",
    "                                                                                      fr_tokenizer)\n",
    "input_data = [en_train, en_test]\n",
    "en_train, en_test = tokenizeInput(input_data, en_tokenizer)\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "print(\"en_vocab {}\\nfr_vocab {}\" .format(en_vocab_size, fr_vocab_size))\n",
    "print(len(en_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_TEXTS\n",
      "That was my idea.  -  C'était mon idée.\n",
      "I was watching TV when the telephone rang.  -  Je regardais la télé lorsque le téléphone a sonné.\n",
      "I want to trust you.  -  Je veux vous faire confiance.\n",
      "I don't know what could've happened.  -  J'ignore ce qui aurait pu arriver.\n",
      "We need to find a new babysitter.  -  Nous devons trouver une nouvelle baby-sitter.\n",
      "I'm still waiting for my breakfast. Bring it to me now, please.  -  J'attends toujours mon petit déjeuner, veuillez me l'apporter maintenant.\n",
      "I know Tom is fast.  -  Je sais que Tom est rapide.\n",
      "I thought they'd heard us.  -  J'ai pensé qu'elles nous avaient entendus.\n",
      "We need to concentrate on coming up with a new plan.  -  Nous devons nous concentrer pour trouver un nouveau plan.\n",
      "It's kind of complicated.  -  C'est plutôt compliqué.\n"
     ]
    }
   ],
   "source": [
    "prediction_idx = np.random.randint(low=40000, high=len(en_lines_raw), size=10)\n",
    "print(\"TEST_TEXTS\")\n",
    "test_text = [(en_lines_raw[idx], fr_lines_raw[idx]) for idx in prediction_idx]\n",
    "for (en,fr) in test_text:\n",
    "    print(en, \" - \", fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['this', 'is', 'a', 'ship']], [['it', 'is', 'ship']], [['ship', 'it', 'is']], [['a', 'ship,', 'it', 'is']]]\n",
      "[['this', 'is', 'a', 'ship'], ['it', 'is', 'ship'], ['ship', 'is'], ['a', 'ship,', 'it']]\n",
      "    4grams 0.5474911439088136\n",
      "    3grams 0.7515945109323554\n",
      "    2grams 0.7918111496765283\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(reference, predicted):\n",
    "    chencherry = bleu.SmoothingFunction()\n",
    "    four_grams_bleu = bleu.corpus_bleu(reference, predicted, smoothing_function=chencherry.method1)\n",
    "    three_grams_bleu = bleu.corpus_bleu(reference, predicted, weights=(1./3., 1./3., 1./3.),  smoothing_function=chencherry.method1)\n",
    "    two_grams_bleu = bleu.corpus_bleu(reference, predicted, weights=(1./2., 1./2.), smoothing_function=chencherry.method1)\n",
    "    print(\"    4grams {}\\n    3grams {}\\n    2grams {}\\n\\n\" .format(four_grams_bleu, three_grams_bleu, two_grams_bleu))\n",
    "\n",
    "reference = [\n",
    "    ['this is a ship'.split()],\n",
    "    ['it is ship'.split()],\n",
    "    ['ship it is'.split()],\n",
    "    ['a ship, it is'.split()] # master Yoda\n",
    "]\n",
    "print(reference)\n",
    "pred = [\n",
    "    'this is a ship'.split(),\n",
    "    'it is ship'.split(),\n",
    "    'ship  is'.split(),\n",
    "    'a ship, it'.split() # master Yoda\n",
    "]\n",
    "print(pred)\n",
    "calculate_bleu(reference, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 256\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Seq2SeqPredictions():\n",
    "    trainer = Seq2SeqTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=20)\n",
    "    losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "    makePlots(losses, accuracy, \"Seq2Seq\")\n",
    "    for (en_text, fr_text) in test_text:\n",
    "        trainer.predict(en_text, fr_text)\n",
    "    _, seq2seqloss = losses\n",
    "    print(\"starting translation...\")\n",
    "    ref = []\n",
    "    pred = []\n",
    "    #for en_text, fr_text in test_text:\n",
    "    for en_text, fr_text in zip(en_lines, fr_lines):\n",
    "        ref.append([[word.lower() for word in fr_text.split()]])\n",
    "        pred.append([word.lower() for word in trainer.translate(en_text)])\n",
    "    print(\"starting BLEU calculation...\")\n",
    "    calculate_bleu(ref, pred)\n",
    "    \n",
    "def Seq2SeqAttentionPredictions():\n",
    "    trainer = Seq2SeqAttentionTrainer(BATCH_SIZE, LSTM_SIZE, EMBEDDING_SIZE, predict_every=20)\n",
    "    losses, accuracy = trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS, \"concat\")\n",
    "    makePlots(losses, accuracy, \"Seq2SeqAttention\")\n",
    "    for (en_text, fr_text) in test_text:\n",
    "        trainer.predict(en_text, fr_text, print_prediction=True)\n",
    "    _, Seq2SeqAttentionloss = losses\n",
    "    print(\"starting translation...\")\n",
    "    ref = []\n",
    "    pred = []\n",
    "    #for en_text, fr_text in test_text:\n",
    "    for en_text, fr_text in zip(en_lines, fr_lines):\n",
    "        ref.append([[word.lower() for word in fr_text.split()]])\n",
    "        pred.append([word.lower() for word in trainer.translate(en_text)])\n",
    "    print(\"starting BLEU calculation...\")\n",
    "    calculate_bleu(ref, pred)\n",
    "\n",
    "def TransformerPredictions():\n",
    "    BATCH_SIZE = 64\n",
    "    num_layers = 6 # 6\n",
    "    d_model = 256 # 512\n",
    "    dff = 512  # 2048\n",
    "    num_heads = 8\n",
    "    trainer = TransformerTrainer(BATCH_SIZE, num_layers, d_model, dff, num_heads, predict_every=20)\n",
    "    losses, accuracy= trainer.train([en_train, fr_train_in, fr_train_out], [en_test, fr_test_in, fr_test_out], [en_lines, fr_lines], [en_tokenizer, fr_tokenizer], EPOCHS)\n",
    "    makePlots(losses, accuracy, \"Transformer\")\n",
    "    for (en_text, fr_text) in test_text:\n",
    "        trainer.predict(en_text, fr_text)\n",
    "    _, Transformerloss = losses\n",
    "    print(\"starting translation...\")\n",
    "    ref = []\n",
    "    pred = []\n",
    "    #for en_text, fr_text in test_text:\n",
    "    for en_text, fr_text in zip(en_lines, fr_lines):\n",
    "        ref.append([[word.lower() for word in fr_text.split()]])\n",
    "        pred.append([word.lower() for word in trainer.translate(en_text)])\n",
    "    print(\"starting BLEU calculation...\")\n",
    "    calculate_bleu(ref, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab 8331\n",
      "fr_vocab 13576\n",
      "Number of devices: 4\n",
      "creating dataset...\n",
      "input :  Tom has lived in Boston for three years .\n",
      "output:  Tom vit a Boston depuis trois ans .\n",
      "training from scratch\n",
      "INFO:tensorflow:batch_all_reduce: 254 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 254 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1 training Loss 81.0891 Accuracy 0.0136  test Loss 75.0595 Accuracy 0.0270\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : Tom has lived in Boston for three years .\n",
      "           Predicted : \n",
      "           Correct   : Tom vit a Boston depuis trois ans .\n",
      "--------------------------END PREDICTION--------------------------\n",
      "Saving checkpoint for epoch 0: ./checkpoints/train/ckpt-1\n",
      "Epoch 2 training Loss 67.2188 Accuracy 0.0161  test Loss 57.5415 Accuracy 0.0270\n",
      "Epoch 3 training Loss 52.1803 Accuracy 0.0277  test Loss 47.5405 Accuracy 0.0628\n",
      "Epoch 4 training Loss 45.1151 Accuracy 0.0412  test Loss 42.1876 Accuracy 0.0772\n",
      "Epoch 5 training Loss 40.2487 Accuracy 0.0483  test Loss 38.0934 Accuracy 0.0863\n",
      "Epoch 6 training Loss 36.3714 Accuracy 0.0532  test Loss 34.8949 Accuracy 0.0947\n",
      "Saving checkpoint for epoch 5: ./checkpoints/train/ckpt-2\n",
      "Epoch 7 training Loss 33.2533 Accuracy 0.0575  test Loss 32.4258 Accuracy 0.1008\n",
      "Epoch 8 training Loss 30.7593 Accuracy 0.0612  test Loss 30.8090 Accuracy 0.1064\n",
      "Epoch 9 training Loss 28.5999 Accuracy 0.0647  test Loss 28.7580 Accuracy 0.1120\n",
      "Epoch 10 training Loss 26.6449 Accuracy 0.0680  test Loss 27.6785 Accuracy 0.1167\n",
      "Epoch 11 training Loss 24.7963 Accuracy 0.0712  test Loss 26.4712 Accuracy 0.1207\n",
      "Saving checkpoint for epoch 10: ./checkpoints/train/ckpt-3\n",
      "Epoch 12 training Loss 23.1250 Accuracy 0.0741  test Loss 25.0865 Accuracy 0.1258\n",
      "Epoch 13 training Loss 21.5486 Accuracy 0.0770  test Loss 24.2183 Accuracy 0.1276\n",
      "Epoch 14 training Loss 20.0526 Accuracy 0.0800  test Loss 23.3115 Accuracy 0.1313\n",
      "Epoch 15 training Loss 18.6517 Accuracy 0.0829  test Loss 22.7137 Accuracy 0.1334\n",
      "Epoch 16 training Loss 17.3548 Accuracy 0.0853  test Loss 21.8482 Accuracy 0.1379\n",
      "Saving checkpoint for epoch 15: ./checkpoints/train/ckpt-4\n",
      "Epoch 17 training Loss 16.1668 Accuracy 0.0880  test Loss 21.5172 Accuracy 0.1391\n",
      "Epoch 18 training Loss 14.9905 Accuracy 0.0906  test Loss 21.0004 Accuracy 0.1409\n",
      "Epoch 19 training Loss 13.9368 Accuracy 0.0931  test Loss 20.8927 Accuracy 0.1408\n",
      "Epoch 20 training Loss 12.9831 Accuracy 0.0952  test Loss 20.2832 Accuracy 0.1441\n",
      "Epoch 21 training Loss 12.0516 Accuracy 0.0975  test Loss 20.2642 Accuracy 0.1460\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : Tom has lived in Boston for three years .\n",
      "           Predicted : tom a vecu trois ans a boston .\n",
      "           Correct   : Tom vit a Boston depuis trois ans .\n",
      "--------------------------END PREDICTION--------------------------\n",
      "Saving checkpoint for epoch 20: ./checkpoints/train/ckpt-5\n",
      "Epoch 22 training Loss 11.2294 Accuracy 0.0999  test Loss 20.2576 Accuracy 0.1467\n",
      "Epoch 23 training Loss 10.4911 Accuracy 0.1017  test Loss 20.3179 Accuracy 0.1469\n",
      "Epoch 24 training Loss 9.8465 Accuracy 0.1038  test Loss 20.4651 Accuracy 0.1480\n",
      "Epoch 25 training Loss 9.1454 Accuracy 0.1059  test Loss 20.3565 Accuracy 0.1479\n",
      "Epoch 26 training Loss 8.6327 Accuracy 0.1075  test Loss 20.8035 Accuracy 0.1483\n",
      "Saving checkpoint for epoch 25: ./checkpoints/train/ckpt-6\n",
      "Epoch 27 training Loss 8.1768 Accuracy 0.1091  test Loss 21.0458 Accuracy 0.1480\n",
      "Epoch 28 training Loss 7.8552 Accuracy 0.1100  test Loss 21.1459 Accuracy 0.1481\n",
      "Epoch 29 training Loss 7.4868 Accuracy 0.1112  test Loss 21.6849 Accuracy 0.1486\n",
      "Epoch 30 training Loss 7.1158 Accuracy 0.1125  test Loss 21.4081 Accuracy 0.1495\n",
      "Epoch 31 training Loss 6.6300 Accuracy 0.1143  test Loss 21.7662 Accuracy 0.1500\n",
      "Saving checkpoint for epoch 30: ./checkpoints/train/ckpt-7\n",
      "Epoch 32 training Loss 6.2344 Accuracy 0.1159  test Loss 21.8833 Accuracy 0.1499\n",
      "Epoch 33 training Loss 5.8870 Accuracy 0.1171  test Loss 22.3259 Accuracy 0.1505\n",
      "Epoch 34 training Loss 5.5314 Accuracy 0.1185  test Loss 22.2950 Accuracy 0.1517\n",
      "Epoch 35 training Loss 5.2354 Accuracy 0.1198  test Loss 22.6052 Accuracy 0.1514\n",
      "Epoch 36 training Loss 4.9476 Accuracy 0.1209  test Loss 22.5917 Accuracy 0.1523\n",
      "Saving checkpoint for epoch 35: ./checkpoints/train/ckpt-8\n",
      "Epoch 37 training Loss 4.6933 Accuracy 0.1221  test Loss 23.2110 Accuracy 0.1518\n",
      "Epoch 38 training Loss 4.4452 Accuracy 0.1231  test Loss 23.1176 Accuracy 0.1521\n",
      "Epoch 39 training Loss 4.2294 Accuracy 0.1240  test Loss 23.2834 Accuracy 0.1527\n",
      "Epoch 40 training Loss 4.0424 Accuracy 0.1249  test Loss 23.9159 Accuracy 0.1513\n",
      "Saving checkpoint for end at ./checkpoints/train/ckpt-9\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : That was my idea.\n",
      "           Predicted : c etait mon intention .\n",
      "           Correct   : C'était mon idée.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I was watching TV when the telephone rang.\n",
      "           Predicted : j ai regarde le televiseur dans la tele .\n",
      "           Correct   : Je regardais la télé lorsque le téléphone a sonné.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I want to trust you.\n",
      "           Predicted : je veux m entretenir .\n",
      "           Correct   : Je veux vous faire confiance.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I don't know what could've happened.\n",
      "           Predicted : je sais ce que ca .\n",
      "           Correct   : J'ignore ce qui aurait pu arriver.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : We need to find a new babysitter.\n",
      "           Predicted : nous devons nous trouver .\n",
      "           Correct   : Nous devons trouver une nouvelle baby-sitter.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I'm still waiting for my breakfast. Bring it to me now, please.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predicted : montre-moi cela que je l attends sur moi .\n",
      "           Correct   : J'attends toujours mon petit déjeuner, veuillez me l'apporter maintenant.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I know Tom is fast.\n",
      "           Predicted : je sais que tom est avec lui .\n",
      "           Correct   : Je sais que Tom est rapide.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : I thought they'd heard us.\n",
      "           Predicted : je pensais que j etais assoiffee .\n",
      "           Correct   : J'ai pensé qu'elles nous avaient entendus.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : We need to concentrate on coming up with a new plan.\n",
      "           Predicted : nous devons nous poser une nouvelle decision .\n",
      "           Correct   : Nous devons nous concentrer pour trouver un nouveau plan.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "----------------------------PREDICTION----------------------------\n",
      "           English   : It's kind of complicated.\n",
      "           Predicted : quel genre de gentil !\n",
      "           Correct   : C'est plutôt compliqué.\n",
      "--------------------------END PREDICTION--------------------------\n",
      "starting translation...\n"
     ]
    }
   ],
   "source": [
    "p = Process(target=TransformerPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = Process(target=Seq2SeqPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = Process(target=Seq2SeqAttentionPredictions, args=())\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = plt.figure()\n",
    "    fig_plot = fig.add_subplot()\n",
    "    fig_plot.plot(seq2seqloss, label=\"seq2seq\")\n",
    "    fig_plot.plot(Seq2SeqAttentionloss, label=\"seq2seqAttention\")\n",
    "    fig_plot.plot(Transformerloss, label=\"Transformer\")\n",
    "    fig_plot.legend(loc=\"upper right\")\n",
    "    fig_plot.set_xlabel(\"epoch\")\n",
    "    fig_plot.set_ylabel(\"loss\")\n",
    "    fig_plot.grid(linestyle=\"--\")\n",
    "    fig.savefig(\"test_losses_plot\" +  \".png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
