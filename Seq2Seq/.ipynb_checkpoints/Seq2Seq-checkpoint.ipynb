{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 250\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"data/fra-eng\", \"fra.txt\")\n",
    "en_lines, fr_lines = zip(*data)\n",
    "\n",
    "fr_train, fr_test, en_train, en_test = train_test_split(fr_lines, en_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "fr_lines_in = ['<start> ' + normalize(line) for line in fr_train]\n",
    "fr_lines_out = [normalize(line) + ' <end>' for line in fr_train]\n",
    "fr_test = [normalize(line) for line in fr_test]\n",
    "\n",
    "en_train = [normalize(line) for line in en_train]\n",
    "en_test = [normalize(line) for line in en_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# padding\n",
    "\n",
    "en_seq, fr_seq_in, fr_seq_out, en_tokenizer, fr_tokenizer = preprocessData(en_train, fr_lines_in, fr_lines_out, fr_test, en_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Decoder network\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_size, units):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.units = units\n",
    "    self.embeding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True, trainable=True)\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(units, dropout=0.2, return_sequences=True, return_state=True)\n",
    "  \n",
    "  def call(self, sequences, lstm_states):\n",
    "    # sequences shape = [batch_size, seq_max_len]\n",
    "    # lstm_states = [batch_size, lstm_size] x 2\n",
    "    # encoder_embedded shape = [batch_size, seq_max_len, embedding_size]\n",
    "    # output shape = [batch_size, seq_max_len, lstm_size]\n",
    "    # state_h, state_c shape = [batch_size, lstm_size] x 2\n",
    "\n",
    "    encoder_embedded = self.embeding_layer(sequences)\n",
    "    #print(\"encoder_embedded = \", encoder_embedded.shape)\n",
    "    output, state_h, state_c = self.lstm_layer(encoder_embedded, initial_state=lstm_states)\n",
    "\n",
    "    return output, state_h, state_c\n",
    "\n",
    "  def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.units]),\n",
    "                tf.zeros([batch_size, self.units]))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_size, units):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(units, dropout=0.2, return_sequences=True,\n",
    "                                           return_state=True)\n",
    "    self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call(self, sequences, lstm_states):\n",
    "    # sequences shape = [batch_size, seq_max_len]\n",
    "    # embedding shape = [batch_size, seq_max_len, embedding_size]\n",
    "    # output shape = [batch_szie, seq_max_len, lstm_size]\n",
    "    # state_h, state_c = [batch_size, lstm_size] x2\n",
    "    # dense shape = [batch_size, seq_max_len, vocab_size]\n",
    "    \n",
    "    decoder_embedded = self.embedding_layer(sequences)\n",
    "    lstm_output, state_h, state_c = self.lstm_layer(decoder_embedded, lstm_states)\n",
    "    return self.dense_layer(lstm_output), state_h, state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder_decoder_shapes():\n",
    "    # checks for encoder state\n",
    "    vocab_size = len(en_tokenizer.word_index)+1\n",
    "    fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "    batch_size = 1\n",
    "    encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "    source_input = tf.constant([[1, 7, 59, 43, 55, 6, 10, 10]])\n",
    "    initial_state = encoder.init_states(batch_size)\n",
    "    encoder_output, en_state_h, en_state_c = encoder(source_input, initial_state)\n",
    "    \n",
    "    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    decoder_input = tf.constant([[1,2,3,4,5]])\n",
    "    decoder_output, de_state_h, de_state_c = decoder(decoder_input, [en_state_h, en_state_c])\n",
    "\n",
    "    assert(decoder_output.shape == (*decoder_input.shape, fr_vocab_size))\n",
    "    assert(de_state_h.shape == (batch_size, LSTM_SIZE))\n",
    "    assert(de_state_c.shape == (batch_size, LSTM_SIZE))\n",
    "\n",
    "    assert(encoder_output.shape == (*source_input.shape, LSTM_SIZE))\n",
    "    assert(en_state_h.shape == (batch_size, LSTM_SIZE))\n",
    "    assert(en_state_c.shape == (batch_size, LSTM_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting step\n",
    "def predict_output():\n",
    "  index = np.random.choice(len(en_test))\n",
    "  en_sentence = en_test[index]\n",
    "  should_be_sentence = fr_test[index]\n",
    "\n",
    "  sentence = en_tokenizer.texts_to_sequences([en_sentence])\n",
    "  initial_states = encoder.init_states(1)\n",
    "  _, state_h, state_c = encoder(tf.constant(sentence), initial_states, training=False)\n",
    "\n",
    "  symbol = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "  sentence = []\n",
    "\n",
    "  while True:\n",
    "    symbol, state_h, state_c = decoder(symbol, (state_h, state_c), training=False)\n",
    "    # argmax to get max index \n",
    "    symbol = tf.argmax(symbol, axis=-1)\n",
    "    word = fr_tokenizer.index_word[symbol.numpy()[0][0]]\n",
    "\n",
    "    if len(sentence) >=23 or word == '<end>':\n",
    "      break\n",
    "\n",
    "    sentence.append(word + \" \")\n",
    "  \n",
    "  predicted_sentence = ''.join(sentence)\n",
    "  print(\"--------------PREDICTION--------------\")\n",
    "  print(\"Predicted sentence:  {} \" .format(predicted_sentence))\n",
    "  print(\"Should be sentence:  {} \" .format(should_be_sentence))\n",
    "  print(\"------------END PREDICTION------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"creating dataset...\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (en_seq, fr_seq_in, fr_seq_out))\n",
    "train_dataset = train_dataset.shuffle(len(en_train)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"dataset created\")\n",
    "print(\"batches each epoch : \", len(en_seq)/BATCH_SIZE)\n",
    "min_loss = 1000000\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "# lost function with zeros masked\n",
    "@tf.function\n",
    "def loss_fn(real, targets):\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # output is softmax result\n",
    "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "  mask = tf.cast(mask, tf.int64)\n",
    "  \n",
    "  return loss(targets, real, sample_weight=mask)\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=5.0)\n",
    "\n",
    "# one training step\n",
    "@tf.function\n",
    "def train_step(encoder_input, decoder_in, target_decoder_out, initial_states):\n",
    "  with tf.GradientTape() as tape:\n",
    "    encoder_states = encoder(encoder_input, initial_state)\n",
    "    decoder_output, _, _ = decoder(decoder_in, encoder_states[1:])\n",
    "\n",
    "    loss = loss_fn(decoder_output, target_decoder_out)\n",
    "  \n",
    "  trainable = encoder.trainable_variables + decoder.trainable_variables\n",
    "  grads = tape.gradient(loss, trainable)\n",
    "  optim.apply_gradients(zip(grads, trainable))\n",
    "\n",
    "  return loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  initial_state = encoder.init_states(BATCH_SIZE)\n",
    "  for batch_nr, (en_input, dec_in, dec_out) in enumerate(train_dataset.take(-1)):\n",
    "    loss = train_step(en_input, dec_in, dec_out, initial_state)\n",
    "  \n",
    "  print(\"current epoch {} - loss {}\" .format(epoch, loss))\n",
    "  try:\n",
    "    predict_output()\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE*strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train dataset...\n",
      "creating test dataset...\n",
      "dataset created\n",
      "batches each epoch :  2399.765625\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "saving weights in epoch  0\n",
      "WARNING:tensorflow:Entity <bound method Tokenizer.texts_to_sequences_generator of <keras_preprocessing.text.Tokenizer object at 0x7f9958205358>> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "WARNING: Entity <bound method Tokenizer.texts_to_sequences_generator of <keras_preprocessing.text.Tokenizer object at 0x7f9958205358>> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "INFO:tensorflow:Error reported to Coordinator: in converted code:\n",
      "\n",
      "    <ipython-input-9-0f6b449a73fa>:52 test_step  *\n",
      "        en_sentence = en_tokenizer.texts_to_sequences([en_text])\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:279 texts_to_sequences  *\n",
      "        return list(self.texts_to_sequences_generator(texts))\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n",
      "        return py_builtins.overload_of(f)(*args)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:310 texts_to_sequences_generator\n",
      "        self.split)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:43 text_to_word_sequence\n",
      "        text = text.lower()\n",
      "\n",
      "    AttributeError: 'int' object has no attribute 'lower'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 879, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 237, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "AttributeError: in converted code:\n",
      "\n",
      "    <ipython-input-9-0f6b449a73fa>:52 test_step  *\n",
      "        en_sentence = en_tokenizer.texts_to_sequences([en_text])\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:279 texts_to_sequences  *\n",
      "        return list(self.texts_to_sequences_generator(texts))\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n",
      "        return py_builtins.overload_of(f)(*args)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:310 texts_to_sequences_generator\n",
      "        self.split)\n",
      "    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:43 text_to_word_sequence\n",
      "        text = text.lower()\n",
      "\n",
      "    AttributeError: 'int' object has no attribute 'lower'\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n\n    <ipython-input-9-0f6b449a73fa>:71 distributed_test_step  *\n        return strategy.experimental_run_v2(test_step, args=(en_text, fr_text,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:760 experimental_run_v2\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    <ipython-input-9-0f6b449a73fa>:52 test_step  *\n        en_sentence = en_tokenizer.texts_to_sequences([en_text])\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:279 texts_to_sequences  *\n        return list(self.texts_to_sequences_generator(texts))\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n        return py_builtins.overload_of(f)(*args)\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:310 texts_to_sequences_generator\n        self.split)\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:43 text_to_word_sequence\n        text = text.lower()\n\n    AttributeError: 'int' object has no attribute 'lower'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0f6b449a73fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0men_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_text\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mdistributed_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" EPOCH : {} loss {} | test loss {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 408\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in converted code:\n\n    <ipython-input-9-0f6b449a73fa>:71 distributed_test_step  *\n        return strategy.experimental_run_v2(test_step, args=(en_text, fr_text,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:760 experimental_run_v2\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    <ipython-input-9-0f6b449a73fa>:52 test_step  *\n        en_sentence = en_tokenizer.texts_to_sequences([en_text])\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:279 texts_to_sequences  *\n        return list(self.texts_to_sequences_generator(texts))\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py:396 converted_call\n        return py_builtins.overload_of(f)(*args)\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:310 texts_to_sequences_generator\n        self.split)\n    /usr/local/lib/python3.6/dist-packages/keras_preprocessing/text.py:43 text_to_word_sequence\n        text = text.lower()\n\n    AttributeError: 'int' object has no attribute 'lower'\n"
     ]
    }
   ],
   "source": [
    "print(\"creating dataset...\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (en_seq, fr_seq_in, fr_seq_out))\n",
    "train_dataset = train_dataset.shuffle(len(en_train)).batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "print(\"dataset created\")\n",
    "print(\"batches each epoch : \", len(en_seq)/BATCH_SIZE)\n",
    "min_loss = 1000000\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "\n",
    "with strategy.scope():\n",
    "    optim = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "    encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    \n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE) # output is softmax result\n",
    "    def compute_loss(predictions, labels):\n",
    "        mask = tf.math.logical_not(tf.math.equal(labels, 0))\n",
    "        mask = tf.cast(mask, tf.int64)\n",
    "        per_example_loss = loss_obj(labels, predictions, sample_weight=mask)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    \n",
    "    # one training step\n",
    "    def train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_states = encoder(encoder_input, initial_state, training=True)\n",
    "            predictions, _, _ = decoder(decoder_in, encoder_states[1:], training=True)\n",
    "            loss = compute_loss(predictions, decoder_out)\n",
    "  \n",
    "        trainable = encoder.trainable_variables + decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable)\n",
    "        optim.apply_gradients(zip(grads, trainable))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step,\n",
    "                                                      args=(encoder_input,\n",
    "                                                            decoder_in,\n",
    "                                                            decoder_out,\n",
    "                                                            initial_states,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        initial_state = encoder.init_states(BATCH_SIZE)\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_nr, (en_input, dec_in, dec_out) in enumerate(train_dataset):\n",
    "            single_loss = distributed_train_step(en_input, dec_in, dec_out, initial_state)\n",
    "            total_loss += single_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        loss = total_loss/num_batches\n",
    "        print(\" EPOCH : {} loss {} \" .format(epoch, loss))\n",
    "        if loss < min_loss:\n",
    "            print(\"saving weights in epoch \", epoch)\n",
    "            encoder.save_weights('saved_models/best_encoder_weights.h5')\n",
    "            decoder.save_weights('saved_models/best_decoder_weights.h5')\n",
    "            min_loss = loss\n",
    "\n",
    "        try:\n",
    "            predict_output()\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
