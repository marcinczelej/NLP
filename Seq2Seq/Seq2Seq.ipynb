{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 250\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.insert(0, r\"../utilities\")\n",
    "\n",
    "from utils import *\n",
    "from model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from  ../data/fra-eng/fra.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "data = read_data(os.path.join(data_dir, \"fra-eng\"), \"fra.txt\")\n",
    "en_lines, fr_lines = zip(*data)\n",
    "\n",
    "en_lines = [normalize(line) for line in en_lines]\n",
    "fr_lines = [normalize(line) for line in fr_lines]\n",
    "\n",
    "fr_train, fr_test, en_train, en_test = train_test_split(fr_lines, en_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "fr_train_in = ['<start> ' + line for line in fr_train]\n",
    "fr_train_out = [line + ' <end>' for line in fr_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = Tokenizer(filters='')\n",
    "en_tokenizer = Tokenizer(filters='')\n",
    "\n",
    "input_data = [fr_train_in, fr_train_out, fr_test]\n",
    "fr_train_in, fr_train_out, fr_test = tokenizeInput(input_data, fr_tokenizer)\n",
    "\n",
    "input_data = [en_train, en_test]\n",
    "en_train, en_test = tokenizeInput(input_data, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE*strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"creating dataset...\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((en_train, fr_train_in, fr_train_out))\n",
    "train_dataset = train_dataset.shuffle(len(en_train)).batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "print(\"dataset created\")\n",
    "print(\"batches each epoch : \", len(en_train)/BATCH_SIZE)\n",
    "min_loss = 1000000\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "\n",
    "with strategy.scope():\n",
    "    optim = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "    encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    \n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE) # output is softmax result\n",
    "    def compute_loss(predictions, labels):\n",
    "        mask = tf.math.logical_not(tf.math.equal(labels, 0))\n",
    "        mask = tf.cast(mask, tf.int64)\n",
    "        per_example_loss = loss_obj(labels, predictions, sample_weight=mask)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "    \n",
    "    # predicting random sentence output\n",
    "    def predict_output():\n",
    "      index = np.random.choice(len(en_test))\n",
    "      en_sentence = en_test[index]\n",
    "      should_be_sentence = fr_test[index]\n",
    "\n",
    "      sentence = en_tokenizer.texts_to_sequences([en_sentence])\n",
    "      initial_states = encoder.init_states(1)\n",
    "      _, state_h, state_c = encoder(tf.constant(sentence), initial_states, training=False)\n",
    "\n",
    "      symbol = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "      sentence = []\n",
    "\n",
    "      while True:\n",
    "        symbol, state_h, state_c = decoder(symbol, (state_h, state_c), training=False)\n",
    "        # argmax to get max index \n",
    "        symbol = tf.argmax(symbol, axis=-1)\n",
    "        word = fr_tokenizer.index_word[symbol.numpy()[0][0]]\n",
    "\n",
    "        if len(sentence) >=23 or word == '<end>':\n",
    "          break\n",
    "\n",
    "        sentence.append(word + \" \")\n",
    "\n",
    "      predicted_sentence = ''.join(sentence)\n",
    "      print(\"--------------PREDICTION--------------\")\n",
    "      print(\"Predicted sentence:  {} \" .format(predicted_sentence))\n",
    "      print(\"Should be sentence:  {} \" .format(should_be_sentence))\n",
    "      print(\"------------END PREDICTION------------\")\n",
    "\n",
    "    # one training step\n",
    "    def train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_states = encoder(encoder_input, initial_state, training=True)\n",
    "            predictions, _, _ = decoder(decoder_in, encoder_states[1:], training=True)\n",
    "            loss = compute_loss(predictions, decoder_out)\n",
    "  \n",
    "        trainable = encoder.trainable_variables + decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable)\n",
    "        optim.apply_gradients(zip(grads, trainable))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step,\n",
    "                                                      args=(encoder_input,\n",
    "                                                            decoder_in,\n",
    "                                                            decoder_out,\n",
    "                                                            initial_states,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        initial_state = encoder.init_states(BATCH_SIZE)\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_nr, (en_data, fr_data_in, fr_data_out) in enumerate(train_dataset):\n",
    "            single_loss = distributed_train_step(en_data, fr_data_in, fr_data_out, initial_state)\n",
    "            total_loss += single_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        loss = total_loss/num_batches\n",
    "        print(\" EPOCH : {} loss {} \" .format(epoch, loss))\n",
    "        if loss < min_loss:\n",
    "            print(\"saving weights in epoch \", epoch)\n",
    "            encoder.save_weights('saved_models/best_encoder_weights.h5')\n",
    "            decoder.save_weights('saved_models/best_decoder_weights.h5')\n",
    "            min_loss = loss\n",
    "\n",
    "        try:\n",
    "            predict_output()\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
