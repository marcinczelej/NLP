{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_SIZE = 512\n",
    "EMBEDDING_SIZE = 250\n",
    "BATCH_SIZE= 64\n",
    "EPOCHS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"data/fra-eng\", \"fra.txt\")\n",
    "en_lines, fr_lines = zip(*data)\n",
    "\n",
    "fr_train, fr_test, en_train, en_test = train_test_split(fr_lines, en_lines, shuffle=True, test_size=0.1)\n",
    "\n",
    "fr_lines_in = ['<start> ' + normalize(line) for line in fr_train]\n",
    "fr_lines_out = [normalize(line) + ' <end>' for line in fr_train]\n",
    "fr_test = [normalize(line) for line in fr_test]\n",
    "\n",
    "en_train = [normalize(line) for line in en_train]\n",
    "en_test = [normalize(line) for line in en_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "# padding\n",
    "\n",
    "en_seq, fr_seq_in, fr_seq_out, en_tokenizer, fr_tokenizer = preprocessData(en_train, fr_lines_in, fr_lines_out, fr_test, en_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Decoder network\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_size, units):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.units = units\n",
    "    self.embeding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size, mask_zero=True, trainable=True)\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(units, dropout=0.2, return_sequences=True, return_state=True)\n",
    "  \n",
    "  def call(self, sequences, lstm_states):\n",
    "    # sequences shape = [batch_size, seq_max_len]\n",
    "    # lstm_states = [batch_size, lstm_size] x 2\n",
    "    # encoder_embedded shape = [batch_size, seq_max_len, embedding_size]\n",
    "    # output shape = [batch_size, seq_max_len, lstm_size]\n",
    "    # state_h, state_c shape = [batch_size, lstm_size] x 2\n",
    "\n",
    "    encoder_embedded = self.embeding_layer(sequences)\n",
    "    #print(\"encoder_embedded = \", encoder_embedded.shape)\n",
    "    output, state_h, state_c = self.lstm_layer(encoder_embedded, initial_state=lstm_states)\n",
    "\n",
    "    return output, state_h, state_c\n",
    "\n",
    "  def init_states(self, batch_size):\n",
    "        return (tf.zeros([batch_size, self.units]),\n",
    "                tf.zeros([batch_size, self.units]))\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_size, units):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "    self.lstm_layer = tf.keras.layers.LSTM(units, dropout=0.2, return_sequences=True,\n",
    "                                           return_state=True)\n",
    "    self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "  \n",
    "  def call(self, sequences, lstm_states):\n",
    "    # sequences shape = [batch_size, seq_max_len]\n",
    "    # embedding shape = [batch_size, seq_max_len, embedding_size]\n",
    "    # output shape = [batch_szie, seq_max_len, lstm_size]\n",
    "    # state_h, state_c = [batch_size, lstm_size] x2\n",
    "    # dense shape = [batch_size, seq_max_len, vocab_size]\n",
    "    \n",
    "    decoder_embedded = self.embedding_layer(sequences)\n",
    "    lstm_output, state_h, state_c = self.lstm_layer(decoder_embedded, lstm_states)\n",
    "    return self.dense_layer(lstm_output), state_h, state_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder_decoder_shapes():\n",
    "    # checks for encoder state\n",
    "    vocab_size = len(en_tokenizer.word_index)+1\n",
    "    fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "    batch_size = 1\n",
    "    encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "    source_input = tf.constant([[1, 7, 59, 43, 55, 6, 10, 10]])\n",
    "    initial_state = encoder.init_states(batch_size)\n",
    "    encoder_output, en_state_h, en_state_c = encoder(source_input, initial_state)\n",
    "    \n",
    "    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    decoder_input = tf.constant([[1,2,3,4,5]])\n",
    "    decoder_output, de_state_h, de_state_c = decoder(decoder_input, [en_state_h, en_state_c])\n",
    "\n",
    "    assert(decoder_output.shape == (*decoder_input.shape, fr_vocab_size))\n",
    "    assert(de_state_h.shape == (batch_size, LSTM_SIZE))\n",
    "    assert(de_state_c.shape == (batch_size, LSTM_SIZE))\n",
    "\n",
    "    assert(encoder_output.shape == (*source_input.shape, LSTM_SIZE))\n",
    "    assert(en_state_h.shape == (batch_size, LSTM_SIZE))\n",
    "    assert(en_state_c.shape == (batch_size, LSTM_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting step\n",
    "def predict_output():\n",
    "  index = np.random.choice(len(en_test))\n",
    "  en_sentence = en_test[index]\n",
    "  should_be_sentence = fr_test[index]\n",
    "\n",
    "  sentence = en_tokenizer.texts_to_sequences([en_sentence])\n",
    "  initial_states = encoder.init_states(1)\n",
    "  _, state_h, state_c = encoder(tf.constant(sentence), initial_states, training=False)\n",
    "\n",
    "  symbol = tf.constant([[fr_tokenizer.word_index['<start>']]])\n",
    "  sentence = []\n",
    "\n",
    "  while True:\n",
    "    symbol, state_h, state_c = decoder(symbol, (state_h, state_c), training=False)\n",
    "    # argmax to get max index \n",
    "    symbol = tf.argmax(symbol, axis=-1)\n",
    "    word = fr_tokenizer.index_word[symbol.numpy()[0][0]]\n",
    "\n",
    "    if len(sentence) >=23 or word == '<end>':\n",
    "      break\n",
    "\n",
    "    sentence.append(word + \" \")\n",
    "  \n",
    "  predicted_sentence = ''.join(sentence)\n",
    "  print(\"--------------PREDICTION--------------\")\n",
    "  print(\"Predicted sentence:  {} \" .format(predicted_sentence))\n",
    "  print(\"Should be sentence:  {} \" .format(should_be_sentence))\n",
    "  print(\"------------END PREDICTION------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"creating dataset...\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (en_seq, fr_seq_in, fr_seq_out))\n",
    "train_dataset = train_dataset.shuffle(len(en_train)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"dataset created\")\n",
    "print(\"batches each epoch : \", len(en_seq)/BATCH_SIZE)\n",
    "min_loss = 1000000\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "\n",
    "# lost function with zeros masked\n",
    "@tf.function\n",
    "def loss_fn(real, targets):\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # output is softmax result\n",
    "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "  mask = tf.cast(mask, tf.int64)\n",
    "  \n",
    "  return loss(targets, real, sample_weight=mask)\n",
    "\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=5.0)\n",
    "\n",
    "# one training step\n",
    "@tf.function\n",
    "def train_step(encoder_input, decoder_in, target_decoder_out, initial_states):\n",
    "  with tf.GradientTape() as tape:\n",
    "    encoder_states = encoder(encoder_input, initial_state)\n",
    "    decoder_output, _, _ = decoder(decoder_in, encoder_states[1:])\n",
    "\n",
    "    loss = loss_fn(decoder_output, target_decoder_out)\n",
    "  \n",
    "  trainable = encoder.trainable_variables + decoder.trainable_variables\n",
    "  grads = tape.gradient(loss, trainable)\n",
    "  optim.apply_gradients(zip(grads, trainable))\n",
    "\n",
    "  return loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  initial_state = encoder.init_states(BATCH_SIZE)\n",
    "  for batch_nr, (en_input, dec_in, dec_out) in enumerate(train_dataset.take(-1)):\n",
    "    loss = train_step(en_input, dec_in, dec_out, initial_state)\n",
    "  \n",
    "  print(\"current epoch {} - loss {}\" .format(epoch, loss))\n",
    "  try:\n",
    "    predict_output()\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE*strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset...\n",
      "dataset created\n",
      "batches each epoch :  2399.765625\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      " EPOCH : 0 loss 41.74510955810547 \n",
      "saving weights in epoch  0\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  il a dit qu il est un peu de la maison .  \n",
      "Should be sentence:  Il a remis son depart jusqu a demain . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 1 loss 29.088356018066406 \n",
      "saving weights in epoch  1\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je ne peux pas me faire de la maison .  \n",
      "Should be sentence:  Je ne peux pas porter tous ces bagages . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 2 loss 23.69782066345215 \n",
      "saving weights in epoch  2\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  quelle est ton nom de ton anniversaire ?  \n",
      "Should be sentence:  Quel est ton numero de telephone ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 3 loss 19.84931182861328 \n",
      "saving weights in epoch  3\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je travaille .  \n",
      "Should be sentence:  Je sens le cafe . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 4 loss 16.821857452392578 \n",
      "saving weights in epoch  4\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  il a fait une carte d oreille .  \n",
      "Should be sentence:  Il s est arrange pour s echapper par la fenetre . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 5 loss 14.397079467773438 \n",
      "saving weights in epoch  5\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom parle francais avec un ami .  \n",
      "Should be sentence:  Tom enseigne le francais a Boston . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 6 loss 12.429835319519043 \n",
      "saving weights in epoch  6\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous partons .  \n",
      "Should be sentence:  Nous abandonnons . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 7 loss 10.847306251525879 \n",
      "saving weights in epoch  7\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  etais tu hier ?  \n",
      "Should be sentence:  Etiez vous occupe hier ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 8 loss 9.577170372009277 \n",
      "saving weights in epoch  8\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je vais arreter de jouer .  \n",
      "Should be sentence:  J arreterai de jouer . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 9 loss 8.53545093536377 \n",
      "saving weights in epoch  9\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  pouvons nous commencer maintenant ?  \n",
      "Should be sentence:  Peut on commencer maintenant ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 10 loss 7.695108890533447 \n",
      "saving weights in epoch  10\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous ne serons pas a boston .  \n",
      "Should be sentence:  Nous ne retournerons pas a Boston . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 11 loss 6.9980950355529785 \n",
      "saving weights in epoch  11\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  il n y avait personne dans la plage .  \n",
      "Should be sentence:  Il n y avait personne sur la plage . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 12 loss 6.397505760192871 \n",
      "saving weights in epoch  12\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je peux survivre .  \n",
      "Should be sentence:  Je peux survivre . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 13 loss 5.892775535583496 \n",
      "saving weights in epoch  13\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous sommes en train d ecouter .  \n",
      "Should be sentence:  Nous ecoutons . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 14 loss 5.453683853149414 \n",
      "saving weights in epoch  14\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j ai su ce que je dis .  \n",
      "Should be sentence:  Je pensais ce que j ai dit . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 15 loss 5.065781116485596 \n",
      "saving weights in epoch  15\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  ils traverserent la frontiere .  \n",
      "Should be sentence:  Elles ont traverse la frontiere . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 16 loss 4.7304534912109375 \n",
      "saving weights in epoch  16\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous sommes ici .  \n",
      "Should be sentence:  Nous sommes ici . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 17 loss 4.423426151275635 \n",
      "saving weights in epoch  17\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous rencontre ici une fois par mois .  \n",
      "Should be sentence:  Nous nous reunissons ici une fois par mois . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 18 loss 4.157001495361328 \n",
      "saving weights in epoch  18\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom a demande a mary de se mettre au premier rang .  \n",
      "Should be sentence:  Tom a demande a Marie de surveiller les enfants . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 19 loss 3.91253662109375 \n",
      "saving weights in epoch  19\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je me suis occupe de tout .  \n",
      "Should be sentence:  J ai pris soin de tout . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 20 loss 3.7050185203552246 \n",
      "saving weights in epoch  20\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  plait il ?  \n",
      "Should be sentence:  Je vous demande pardon ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 21 loss 3.5025696754455566 \n",
      "saving weights in epoch  21\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  il est difficile d avoir oublie ce qui s est passe .  \n",
      "Should be sentence:  C est difficile d oublier ce qui s est passe . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 22 loss 3.328843832015991 \n",
      "saving weights in epoch  22\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom a laisse les lumieres allumees la nuit .  \n",
      "Should be sentence:  Tom a laisse la lumiere allumee toute la nuit . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 23 loss 3.175001382827759 \n",
      "saving weights in epoch  23\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  c est un type de filles .  \n",
      "Should be sentence:  C est un type paresseux . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 24 loss 3.022782325744629 \n",
      "saving weights in epoch  24\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  une voiture a un volant .  \n",
      "Should be sentence:  Une bagnole a un volant . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 25 loss 2.8970329761505127 \n",
      "saving weights in epoch  25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  il vous faut prendre pour votre sante droit de la sante .  \n",
      "Should be sentence:  Vous devez adjoindre votre photo au formulaire de candidature . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 26 loss 2.7707223892211914 \n",
      "saving weights in epoch  26\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  marie boucla ses cheveux avec un fer a friser .  \n",
      "Should be sentence:  Marie a ondule ses cheveux avec un fer a friser . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 27 loss 2.660975456237793 \n",
      "saving weights in epoch  27\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je veux savoir ce que tu veux .  \n",
      "Should be sentence:  Je veux vouloir ce que tu veux . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 28 loss 2.568042039871216 \n",
      "saving weights in epoch  28\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tu es dotee d une grande imagination .  \n",
      "Should be sentence:  Vous etes dotes d une grande imagination . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 29 loss 2.4758520126342773 \n",
      "saving weights in epoch  29\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  entre vous en !  \n",
      "Should be sentence:  Entrez ! \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 30 loss 2.395625114440918 \n",
      "saving weights in epoch  30\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j ai plante un pin .  \n",
      "Should be sentence:  J ai plante un arbre . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 31 loss 2.3206679821014404 \n",
      "saving weights in epoch  31\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je voulais que tu aies ceci .  \n",
      "Should be sentence:  Je voulais que vous ayez ceci . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 32 loss 2.2455220222473145 \n",
      "saving weights in epoch  32\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom a ignore toute la matinee a mary .  \n",
      "Should be sentence:  Tom a ignore Mary toute la matinee . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 33 loss 2.186467409133911 \n",
      "saving weights in epoch  33\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  quelle idee splendide !  \n",
      "Should be sentence:  Quelle idee fantastique ! \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 34 loss 2.131321668624878 \n",
      "saving weights in epoch  34\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tout se passe sans toi .  \n",
      "Should be sentence:  Tout est mieux sans toi . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 35 loss 2.074904203414917 \n",
      "saving weights in epoch  35\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  enchante de faire votre connaissance .  \n",
      "Should be sentence:  Ravie de faire ta connaissance . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 36 loss 2.0315675735473633 \n",
      "saving weights in epoch  36\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je voulais en discuter avec toi hier mais tu ne semblais pas vouloir ecouter .  \n",
      "Should be sentence:  Je voulais en discuter avec vous hier mais vous ne sembliez pas disposes a ecouter . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 37 loss 1.9815090894699097 \n",
      "saving weights in epoch  37\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je l accepte en son invitation .  \n",
      "Should be sentence:  J ai accepte son invitation . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 38 loss 1.9449599981307983 \n",
      "saving weights in epoch  38\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  en etes vous vraiment sur ?  \n",
      "Should be sentence:  En es tu vraiment sure ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 39 loss 1.9016270637512207 \n",
      "saving weights in epoch  39\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  comment avez vous appris l allemand ?  \n",
      "Should be sentence:  Comment as tu appris l allemand ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 40 loss 1.8694126605987549 \n",
      "saving weights in epoch  40\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tu ferais mieux de ne pas dormir dans le cafe de nuit d ici .  \n",
      "Should be sentence:  Tu ferais mieux de ne pas boire autant de cafe si tard le soir . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 41 loss 1.8290499448776245 \n",
      "saving weights in epoch  41\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  les arbres de sexe furent vains de l environnement .  \n",
      "Should be sentence:  La fourrure fournit aux animaux une protection contre le froid . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 42 loss 1.8037645816802979 \n",
      "saving weights in epoch  42\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j ai trouve necessaire qu il arrive tot demain .  \n",
      "Should be sentence:  J ai estime necessaire de me lever tot chaque matin . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 43 loss 1.7763510942459106 \n",
      "saving weights in epoch  43\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j ai une paire de lunettes de soleil .  \n",
      "Should be sentence:  J ai une paire de lunettes noires . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 44 loss 1.7495194673538208 \n",
      "saving weights in epoch  44\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je ne veux pas que quiconque nous voie .  \n",
      "Should be sentence:  Je ne veux pas que quelqu un nous voie . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 45 loss 1.7281123399734497 \n",
      "saving weights in epoch  45\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  elle ne fume pas .  \n",
      "Should be sentence:  Elle ne fume pas . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 46 loss 1.696454405784607 \n",
      "saving weights in epoch  46\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je peux t apprendre a les demander .  \n",
      "Should be sentence:  Je peux t enseigner a prier . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 47 loss 1.6840269565582275 \n",
      "saving weights in epoch  47\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je perds patience avec vous .  \n",
      "Should be sentence:  Je perds patience avec toi . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 48 loss 1.6576874256134033 \n",
      "saving weights in epoch  48\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  qu est ce qui vous a effraye ?  \n",
      "Should be sentence:  Qu est ce qui vous a effrayee ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 49 loss 1.6380096673965454 \n",
      "saving weights in epoch  49\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  vous devriez travailler dur .  \n",
      "Should be sentence:  Tu devrais travailler dur . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 50 loss 1.624524712562561 \n",
      "saving weights in epoch  50\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  nous avons de la place pour des annees de dollars .  \n",
      "Should be sentence:  Nous avons de la place pour une trentaine de personnes . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 51 loss 1.6017624139785767 \n",
      "saving weights in epoch  51\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom n a pas de chance .  \n",
      "Should be sentence:  Tom n a pas de chance . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 52 loss 1.5876569747924805 \n",
      "saving weights in epoch  52\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je les ai vite rattrapees .  \n",
      "Should be sentence:  Je les ai vite rattrapes . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 53 loss 1.5764433145523071 \n",
      "saving weights in epoch  53\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  personne ne m a parle avec .  \n",
      "Should be sentence:  Personne n a parle avec moi . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 54 loss 1.562744379043579 \n",
      "saving weights in epoch  54\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  a peu pres combien de livres detiens tu ?  \n",
      "Should be sentence:  A peu pres combien de livres possedes tu ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 55 loss 1.5532152652740479 \n",
      "saving weights in epoch  55\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j espere ne pas me voir danser .  \n",
      "Should be sentence:  J espere que personne ne m a vu danser . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 56 loss 1.5352245569229126 \n",
      "saving weights in epoch  56\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je suis desole je ne vous reconnais pas .  \n",
      "Should be sentence:  Je suis desolee je ne vous reconnais pas . \n",
      "------------END PREDICTION------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EPOCH : 57 loss 1.519309401512146 \n",
      "saving weights in epoch  57\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je pense souvent a ma mere morte .  \n",
      "Should be sentence:  Je pense souvent a ma defunte mere . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 58 loss 1.510385274887085 \n",
      "saving weights in epoch  58\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  j ai ete pris de vertiges .  \n",
      "Should be sentence:  J ai ete prise de vertiges . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 59 loss 1.494638442993164 \n",
      "saving weights in epoch  59\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  mon pere est occupe .  \n",
      "Should be sentence:  Mon pere est occupe . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 60 loss 1.4888074398040771 \n",
      "saving weights in epoch  60\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tom est un beau gars .  \n",
      "Should be sentence:  Tom est un beau mec . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 61 loss 1.483810305595398 \n",
      "saving weights in epoch  61\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tu ne veux pas me dire pourquoi si ?  \n",
      "Should be sentence:  Vous ne voulez pas me dire pourquoi si ? \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 62 loss 1.4699783325195312 \n",
      "saving weights in epoch  62\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  tu as dit que tu allais acheter une nouvelle voiture .  \n",
      "Should be sentence:  Tu as dit que tu allais acheter une nouvelle voiture . \n",
      "------------END PREDICTION------------\n",
      " EPOCH : 63 loss 1.4573439359664917 \n",
      "saving weights in epoch  63\n",
      "--------------PREDICTION--------------\n",
      "Predicted sentence:  je lui ai ecrit apres le nom et je l ai dit .  \n",
      "Should be sentence:  J ai ecrit son nom pour ne pas l oublier . \n",
      "------------END PREDICTION------------\n"
     ]
    }
   ],
   "source": [
    "print(\"creating dataset...\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (en_seq, fr_seq_in, fr_seq_out))\n",
    "train_dataset = train_dataset.shuffle(len(en_train)).batch(GLOBAL_BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "print(\"dataset created\")\n",
    "print(\"batches each epoch : \", len(en_seq)/BATCH_SIZE)\n",
    "min_loss = 1000000\n",
    "\n",
    "vocab_size = len(en_tokenizer.word_index)+1\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "\n",
    "with strategy.scope():\n",
    "    optim = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
    "    encoder = Encoder(vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)\n",
    "    \n",
    "    loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE) # output is softmax result\n",
    "    def compute_loss(predictions, labels):\n",
    "        mask = tf.math.logical_not(tf.math.equal(labels, 0))\n",
    "        mask = tf.cast(mask, tf.int64)\n",
    "        per_example_loss = loss_obj(labels, predictions, sample_weight=mask)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "    \n",
    "    # one training step\n",
    "    def train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoder_states = encoder(encoder_input, initial_state, training=True)\n",
    "            predictions, _, _ = decoder(decoder_in, encoder_states[1:], training=True)\n",
    "            loss = compute_loss(predictions, decoder_out)\n",
    "  \n",
    "        trainable = encoder.trainable_variables + decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable)\n",
    "        optim.apply_gradients(zip(grads, trainable))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def distributed_train_step(encoder_input, decoder_in, decoder_out, initial_states):\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step,\n",
    "                                                      args=(encoder_input,\n",
    "                                                            decoder_in,\n",
    "                                                            decoder_out,\n",
    "                                                            initial_states,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                           axis=None)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        initial_state = encoder.init_states(BATCH_SIZE)\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch_nr, (en_input, dec_in, dec_out) in enumerate(train_dataset):\n",
    "            single_loss = distributed_train_step(en_input, dec_in, dec_out, initial_state)\n",
    "            total_loss += single_loss\n",
    "            num_batches += 1\n",
    "\n",
    "        loss = total_loss/num_batches\n",
    "        print(\" EPOCH : {} loss {} \" .format(epoch, loss))\n",
    "        if loss < min_loss:\n",
    "            print(\"saving weights in epoch \", epoch)\n",
    "            encoder.save_weights('saved_models/best_encoder_weights.h5')\n",
    "            decoder.save_weights('saved_models/best_decoder_weights.h5')\n",
    "            min_loss = loss\n",
    "\n",
    "        try:\n",
    "            predict_output()\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
