{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "160XBLnmv-sP",
        "colab_type": "code",
        "outputId": "1e512f76-6c36-42f8-dc8a-9768ed78c200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.3)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHsXhZrJv_cX",
        "colab_type": "code",
        "outputId": "b7282239-9f4b-4d4e-89c2-93ccc2775d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyz0SYVFAIMW",
        "colab_type": "code",
        "outputId": "8ae01144-ba5f-4718-8e61-52cd55224a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.90648814 0.9491688 ]\n",
            " [0.09278559 0.39020795]\n",
            " [0.66258825 0.47905481]]\n",
            "[[0.90648814 0.09278559 0.66258825]\n",
            " [0.9491688  0.39020795 0.47905481]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5nysTjiwDfT",
        "colab_type": "code",
        "outputId": "b91ef26e-f459-4a35-e5a3-164eab21b90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "nx, ny = (8,4)\n",
        "print(np.arange(nx))\n",
        "print(np.arange(ny))\n",
        "p, i = np.meshgrid(np.arange(nx), np.arange(ny))\n",
        "print(\"p = \", p)\n",
        "print(\"i = \", i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7]\n",
            "[0 1 2 3]\n",
            "p =  [[0 1 2 3 4 5 6 7]\n",
            " [0 1 2 3 4 5 6 7]\n",
            " [0 1 2 3 4 5 6 7]\n",
            " [0 1 2 3 4 5 6 7]]\n",
            "i =  [[0 0 0 0 0 0 0 0]\n",
            " [1 1 1 1 1 1 1 1]\n",
            " [2 2 2 2 2 2 2 2]\n",
            " [3 3 3 3 3 3 3 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCWpOaAuxzcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
        "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
        "        tf.print(\"p shape \", p.shape, \"  p = \", p)\n",
        "        tf.print(\"i shape \", i.shape, \"  i = \", i)\n",
        "        pos_emb = np.empty((1, max_steps, max_dims))\n",
        "        tf.print(\"pos_emb shape \", pos_emb.shape)\n",
        "        sins = np.sin(p/10000**(2*i/max_dims))\n",
        "        coss = np.cos(p/10000**(2*i/max_dims))\n",
        "        tf.print(\"sins shape {}   Transposed shape {}\" .format(sins.shape, sins.T.shape))\n",
        "        tf.print(\"coss shape {}   Transposed shape {}\" .format(coss.shape, coss.T.shape))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
        "    def call(self, inputs):\n",
        "        return self.positional_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITxqYjCe1QY1",
        "colab_type": "code",
        "outputId": "621c75ba-b433-4fab-adaf-936c082a9bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "embed_size = 10; max_steps = 3; vocab_size = 100\n",
        "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
        "res = positional_encoding([1,2,3,4])\n",
        "print(res)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p shape  (5, 3)   p =  array([[0, 1, 2],\n",
            "       [0, 1, 2],\n",
            "       [0, 1, 2],\n",
            "       [0, 1, 2],\n",
            "       [0, 1, 2]])\n",
            "i shape  (5, 3)   i =  array([[0, 0, 0],\n",
            "       [1, 1, 1],\n",
            "       [2, 2, 2],\n",
            "       [3, 3, 3],\n",
            "       [4, 4, 4]])\n",
            "pos_emb shape  (1, 3, 10)\n",
            "sins shape (5, 3)   Transposed shape (3, 5)\n",
            "coss shape (5, 3)   Transposed shape (3, 5)\n",
            "tf.Tensor(\n",
            "[[[ 0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "    0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "    0.0000000e+00  1.0000000e+00]\n",
            "  [ 8.4147096e-01  5.4030228e-01  1.5782665e-01  9.8746681e-01\n",
            "    2.5116222e-02  9.9968451e-01  3.9810613e-03  9.9999207e-01\n",
            "    6.3095731e-04  9.9999982e-01]\n",
            "  [ 9.0929741e-01 -4.1614684e-01  3.1169716e-01  9.5018148e-01\n",
            "    5.0216600e-02  9.9873835e-01  7.9620592e-03  9.9996829e-01\n",
            "    1.2619144e-03  9.9999923e-01]]], shape=(1, 3, 10), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_4Bkpw11pU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncodingSimpleLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_dim, max_sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingSimpleLayer, self).__init__(dtype=tf.float32, **kwargs)\n",
        "    if embedding_dim %2 != 0:\n",
        "      embedding_dim+=1\n",
        "    PE = np.zeros((1, max_sentence_len, embedding_dim))\n",
        "    for pos in range(max_sentence_len):\n",
        "      for i in range(embedding_dim//2):\n",
        "        PE[:, pos, 2*i] = np.sin(pos/10000**(2*i/embedding_dim))\n",
        "        PE[:, pos, 2*i+1] = np.cos(pos/10000**(2*i/embedding_dim))\n",
        "    tf.print(PE.shape)\n",
        "    self.PE = PE\n",
        "  def call(self, input):\n",
        "    return self.PE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul4lfwi89x6T",
        "colab_type": "code",
        "outputId": "ce91c00d-5664-4224-9b6c-a3ad2c74a6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "positional_encoding_layer = PositionalEncodingSimpleLayer(embed_size, max_sentence_len=max_steps)\n",
        "res2 = positional_encoding_layer([1,2,3,4,5])\n",
        "print(res2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "[[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00]\n",
            "  [ 8.41470985e-01  5.40302306e-01  1.57826640e-01  9.87466836e-01\n",
            "    2.51162229e-02  9.99684538e-01  3.98106119e-03  9.99992076e-01\n",
            "    6.30957303e-04  9.99999801e-01]\n",
            "  [ 9.09297427e-01 -4.16146837e-01  3.11697146e-01  9.50181503e-01\n",
            "    5.02165994e-02  9.98738351e-01  7.96205928e-03  9.99968302e-01\n",
            "    1.26191435e-03  9.99999204e-01]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW8DXWQl92-S",
        "colab_type": "code",
        "outputId": "4b459ed5-e9b1-4117-efec-334611861800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"comparing arrays: \", np.allclose(res, res2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comparing arrays:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_EB-6u-2Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncodingArangePos(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, max_sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingArangePos, self).__init__(dtype, **kwargs)\n",
        "    if embedding_size%2 !=0:\n",
        "      embedding_size+=1\n",
        "    PE = np.zeros((1, max_sentence_len, embedding_size))\n",
        "    print(PE.shape)\n",
        "    pos = np.arange(start=0, stop=max_sentence_len, step=1)\n",
        "    print(pos.shape)\n",
        "    for i in range(embedding_size//2):\n",
        "      PE[0, ::, 2*i] = np.sin(pos/10000**(2*i/embedding_size))\n",
        "      PE[0, ::, 2*i+1] = np.cos(pos/10000**(2*i/embedding_size))\n",
        "    self.PE = PE\n",
        "  def call(self, inputs):\n",
        "    return self.PE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th4cZj04BZuB",
        "colab_type": "code",
        "outputId": "a885b3e6-e90e-44af-e15f-de37d7e54220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "peLayer = PositionalEncodingArangePos(embed_size, max_sentence_len=max_steps)\n",
        "res3 = peLayer([1,2,3,4,5])\n",
        "print(res3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "(3,)\n",
            "[[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00]\n",
            "  [ 8.41470985e-01  5.40302306e-01  1.57826640e-01  9.87466836e-01\n",
            "    2.51162229e-02  9.99684538e-01  3.98106119e-03  9.99992076e-01\n",
            "    6.30957303e-04  9.99999801e-01]\n",
            "  [ 9.09297427e-01 -4.16146837e-01  3.11697146e-01  9.50181503e-01\n",
            "    5.02165994e-02  9.98738351e-01  7.96205928e-03  9.99968302e-01\n",
            "    1.26191435e-03  9.99999204e-01]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jeicAkABlgx",
        "colab_type": "code",
        "outputId": "f4de8888-55ed-4b93-ddbd-3d05876df88f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"comparing arrays: \", np.allclose(res, res3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comparing arrays:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPepsYlhFzB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQmC7VrjGMQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ADeR0AGPsQ",
        "colab_type": "code",
        "outputId": "744f80a1-a232-4968-9e55-ba0c2149f304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "Encoder flow :\n",
        "\n",
        "- Embedding \n",
        "- Positional Encoding\n",
        "- Input = Embedding + Positional Encoding\n",
        "--------------------REPEAT N Times--------------------\n",
        "- Multi-head Attention layer\n",
        "- Input + Multi-Head Attention layer added together \n",
        "- previous Normalized (1)\n",
        "- Feed Forward Network (2)\n",
        "- (1) added to (2) and Normmalized\n",
        "------------------------------------------------------\n",
        "- Encoder output \n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEncoder flow :\\n\\n- Embedding \\n- Positional Encoding\\n- Input = Embedding + Positional Encoding\\n--------------------REPEAT N Times--------------------\\n- Multi-head Attention layer\\n- Input + Multi-Head Attention layer added together \\n- previous Normalized (1)\\n- Feed Forward Network (2)\\n- (1) added to (2) and Normmalized\\n------------------------------------------------------\\n- Encoder output \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI8KlrtP_ay8",
        "colab_type": "code",
        "outputId": "874e8678-09e7-498f-dac2-1bb4401ca59b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingLayer, self).__init__(dtype, **kwargs)\n",
        "    if embedding_size%2 !=0:\n",
        "      embedding_size+=1\n",
        "    # embedding size -> depth of model\n",
        "    # positional encoding should have size : [1, sentence_len, embedding_size]\n",
        "    # 1 is here to make broadcasting possible in call method\n",
        "    PE = np.zeros((1, sentence_len, embedding_size))\n",
        "    # pos should have shape [1, sentence_len] with values <0, sentence_len)\n",
        "    pos = np.arange(start=0, stop=sentence_len, step=1)\n",
        "    pos = pos.reshape(sentence_len, 1)\n",
        "    # i should have shappe [1, embedding_size//2] with values <0, embedding_size//2)\n",
        "    # we need half of embedding size, because half is needed for each sin/cos \n",
        "    # then we put it together into PE and we have [1, sentence_len, embedding_size]\n",
        "    i = np.arange(start=0, stop=embedding_size//2, step=1)\n",
        "    i = i.reshape(embedding_size//2, 1).T\n",
        "    PE_sin = np.sin(pos/10000**(2*i/embedding_size))\n",
        "    PE_cos = np.cos(pos/10000**(2*i/embedding_size))\n",
        "    # we put sin into even indexes ::2 \n",
        "    # we put cos into odd indexes, thats why we`re starting from 1 here : 1::2\n",
        "    PE[0, ::, ::2] = PE_sin\n",
        "    PE[0, ::, 1::2] = PE_cos\n",
        "    self.PE = tf.constant(PE, dtype=dtype)\n",
        "  def getPE(self):\n",
        "    \"\"\"\n",
        "    only for debuging purposes\n",
        "    \"\"\"\n",
        "    return self.PE\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    inputs shape should be same as self.PE shape\n",
        "    In case that this is not assured please add:\n",
        "    \n",
        "      input_shape = tf.shape(inputs)\n",
        "      return inputs + self.PE[:, input_shape[-2], input_shape[-1]]\n",
        "\n",
        "    instead of below return line\n",
        "    \"\"\"\n",
        "    return inputs + self.PE[:,:,:]\n",
        "\n",
        "peLayerAll = PositionalEncodingLayer(embed_size, sentence_len=max_steps)\n",
        "res4 = peLayerAll.getPE()\n",
        "print(res4.shape)\n",
        "print(\"comparing arrays: \", np.allclose(res, res4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "comparing arrays:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqxOYEMvhs4n",
        "colab_type": "code",
        "outputId": "692b4f5b-e9c0-460c-e4bb-50dfc4762357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, heads_number, dtype=tf.float32, **kwargs):\n",
        "    super(MultiHeadAttentionLayer, self).__init__(dtype=tf.float32, **kwargs)\n",
        "    \"\"\"\n",
        "    return shape : [batch_size, sequence_len, d_model]\n",
        "    heads_number - tell how many heads will be processed at same time\n",
        "    d_model - model size ; equal to embedding_size\n",
        "    \"\"\"\n",
        "    self.heads_number = heads_number\n",
        "    self.d_model = embedding_size\n",
        "    self.w_q = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_k = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_v = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    self.outputLayer = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  # similar to dot attention but with scaling added\n",
        "  def ScaledDotProductAttention(self, v, k, q, sequence_mask):\n",
        "    \"\"\"\n",
        "    q shape [batch_size, num_heads, q_seq_len, depth_q]\n",
        "    k shape [batch_size, num_heads, k_seq_len, depth_k]\n",
        "    v shape [batch_size, num_heads, v_seq_len, depth_v]\n",
        "    \"\"\"\n",
        "    # matmul(q,k,v)\n",
        "    # resultion shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    qk_matmul = tf.matmul(q, k, transpose_b=True)\n",
        "    # scaling tf.cast is needed here because tf.sqrt needs float32 type\n",
        "    # score shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    score = qk_matmul*tf.math.sqrt(tf.cast(k.shape[-1], dtype=tf.float32))\n",
        "    # optional mask\n",
        "    # mask should be shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    # for example [\n",
        "    #             [0, 1, 1]\n",
        "    #             [0, 0, 1]\n",
        "    #             ] shape == (2, 3)\n",
        "    # we`re adding big negative number, because we only care about present/past words that are przedicted\n",
        "    print(sequence_mask)\n",
        "    if sequence_mask != None:\n",
        "      print(\" mask is not none\")\n",
        "      score += sequence_mask*-1e-8\n",
        "    # softmax\n",
        "    # attention_weights shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    attention_weights = tf.nn.softmax(score, axis=-1)\n",
        "    # matmul(res, V)\n",
        "    # contex shape [batch_size, num_heads, q_seq_len, depth_v]\n",
        "    context = tf.matmul(attention_weights, v)\n",
        "    return context\n",
        "\n",
        "  def splitHeads(self, data):\n",
        "    # new shape [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "    data = tf.reshape(data, (data.shape[0], data.shape[1], self.heads_number, data.shape[-1]//self.heads_number))\n",
        "    # transpose dimentions to [batch_size, heads_number, sequence_len, d_model//heads_number]\n",
        "    return tf.transpose(data, perm=[0,2,1,3])\n",
        "\n",
        "  def call(self, q, k, v, sequence_mask):\n",
        "    \"\"\"\n",
        "    q shape [batch_size, sequence_len, d_model]\n",
        "    k shape [batch_size, sequence_len, d_model]\n",
        "    v shape [batch_size, sequence_len, d_model]\n",
        "\n",
        "    after first operations shapes are the same\n",
        "    next we have to split d_model into heads_number of subbatches\n",
        "    new shape after reshape only should be : [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "    next shape should be transposed to : [batch_size, heads_number, sequence_len, d_model//heads_number]\n",
        "    where :\n",
        "      new_d_model = d_model/heads_number\n",
        "    \n",
        "    next make scaled dot-product attention on resulting q,k,v\n",
        "\n",
        "    next concat returning data to get shape : [batch_size, sequence_len, d_model]\n",
        "    in order to do this we have to transpose context_vector to get [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "\n",
        "    next put it throug dense layer (d_model) in order to get output\n",
        "    \"\"\"\n",
        "    print(\"q shape {}\\nk shape {}\\n v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "    q = self.w_q(q)\n",
        "    k = self.w_k(k)\n",
        "    v = self.w_v(v)\n",
        "    print(\"AFTER Dense\\n  q shape {}\\n  k shape {}\\n  v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "\n",
        "    q = self.splitHeads(q)\n",
        "    k = self.splitHeads(k)\n",
        "    v = self.splitHeads(v)\n",
        "    print(\"AFTER SPLIT\\n  q shape {}\\n  k shape {}\\n  v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "\n",
        "    context_vector = self.ScaledDotProductAttention(q, k, v, sequence_mask)\n",
        "    print(\"context_vector shape :\", context_vector.shape)\n",
        "\n",
        "    context_vector = tf.transpose(context_vector, perm=[0,2,1,3])\n",
        "    print(\"context_vector  transposed shape :\", context_vector.shape)\n",
        "    context_vector = tf.reshape(context_vector, (context_vector.shape[0], context_vector.shape[1], self.d_model))\n",
        "    print(\"context_vector  reshapeed shape :\", context_vector.shape)\n",
        "\n",
        "    return self.outputLayer(context_vector)\n",
        "\n",
        "embed_size = 10; max_steps = 3; vocab_size = 100\n",
        "\n",
        "q = tf.random.uniform((1, max_steps, embed_size))  # shape [batch_size, sequence_len, embedding_size]\n",
        "mhatt = MultiHeadAttentionLayer(embed_size, 5)\n",
        "mhatt_output = mhatt(q, k=q, v=q, sequence_mask=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "q shape (1, 3, 10)\n",
            "k shape (1, 3, 10)\n",
            " v shape (1, 3, 10)\n",
            "AFTER Dense\n",
            "  q shape (1, 3, 10)\n",
            "  k shape (1, 3, 10)\n",
            "  v shape (1, 3, 10)\n",
            "AFTER SPLIT\n",
            "  q shape (1, 5, 3, 2)\n",
            "  k shape (1, 5, 3, 2)\n",
            "  v shape (1, 5, 3, 2)\n",
            "None\n",
            "context_vector shape : (1, 5, 3, 2)\n",
            "context_vector  transposed shape : (1, 3, 5, 2)\n",
            "context_vector  reshapeed shape : (1, 3, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4GNTftVHb5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feedForwardnetwork(dff, d_model):\n",
        "  \"\"\"\n",
        "  according to paper dff=2048 and d_model =512\n",
        "  but d_model should be same as embedding_size/d_model in MultiHeadAttention\n",
        "  ffn(x) = max(0, xW_1 + b+1)W_2 + b_2\n",
        "  where max(0, ...) -> relu activation\n",
        "  \"\"\"\n",
        "  ffNetwork = tf.keras.Sequential()\n",
        "  ffNetwork.add(tf.keras.layers.Dense(dff, activation=\"relu\"))\n",
        "  ffNetwork.add(tf.keras.layers.Dense(d_model))\n",
        "  return ffNetwork\n",
        "\n",
        "def makeSequenceMask(seq_len):\n",
        "  \"\"\"\n",
        "  mask should be size [1, 1, seq_len, seq_len]\n",
        "  first two sizes are batch_szie, num_heads to make this matrix broadcastable\n",
        "  it should be in form \n",
        "  [\n",
        "    [0, 1, 1, 1]\n",
        "    [0, 0, 1, 1]\n",
        "    [0, 0, 0, 1]\n",
        "    [0, 0, 0, 0]\n",
        "  ]\n",
        "  \"\"\"\n",
        "  mask_array = np.ones((seq_len, seq_len))\n",
        "  mask_array = np.triu(mask_array, 1)\n",
        "  return tf.reshape(mask_array, (1, 1, *mask_array.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-_p_e0UNCpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_size, max_sentence_len, vocab_size, blocks_amount, heads_number):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    assert (embedding_size//heads_number)%2==0\n",
        "    self.d_model = embedding_size\n",
        "    self.blocks_amount = blocks_amount\n",
        "    self.seq_len = max_sentence_len\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.positionalEncoding = PositionalEncodingLayer(embedding_size, max_sentence_len)\n",
        "    self.multiHeadAttention = MultiHeadAttentionLayer(embedding_size, heads_number)\n",
        "    self.ffNetwork = feedForwardnetwork(1024, self.d_model)\n",
        "\n",
        "    self.w_q = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_k = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_v = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    self.normalizationFirst = tf.keras.layers.LayerNormalization()\n",
        "    self.normalizationSecond = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, sequence):\n",
        "    # sequence shape [batch_size, max_sentence_len]\n",
        "    embedded_seq = self.embedding(sequence)\n",
        "    # embedded_seq shape [batch_szie, max_sentence_len, embedding_size]\n",
        "    block_input = self.positionalEncoding(embedded_seq)\n",
        "    #------------------------- loop though all blocks -------------------------\n",
        "    for i in range(self.blocks_amount):\n",
        "      shortcut_data = block_input\n",
        "      # shortcut_data shape [batch_szie, max_sentence_len, embedding_size]\n",
        "\n",
        "      q = self.w_q(block_input)\n",
        "      k = self.w_k(block_input)\n",
        "      v = self.w_v(block_input)\n",
        "\n",
        "      mask = makeSequenceMask(self.seq_len)\n",
        "      \n",
        "      # mhatt_output shape [batch_size, max_sentence_len, embedding_size]\n",
        "      mhatt_output = self.multiHeadAttention(v, k, q, mask)\n",
        "      print(\"mhatt_output shape \", mhatt_output.shape)\n",
        "\n",
        "      # add & Norm\n",
        "      mhatt_output += shortcut_data\n",
        "      mhatt_output = self.normalizationFirst(mhatt_output)\n",
        "\n",
        "      shortcut_data = mhatt_output\n",
        "      \n",
        "      # put Feed forward ntwork here\n",
        "      ffn_output = self.ffNetwork(mhatt_output)\n",
        "      ffn_output += shortcut_data\n",
        "      block_output = self.normalizationSecond(ffn_output)\n",
        "      block_input = block_output\n",
        "\n",
        "    return block_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T85ncKrVRYC",
        "colab_type": "code",
        "outputId": "398831f0-822e-4991-e648-e3704dddb4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        }
      },
      "source": [
        "data = np.ones((32, max_steps))\n",
        "print(\"input shape \", data.shape)\n",
        "blocks_amount = 2\n",
        "heads = 5\n",
        "encoder = Encoder(embed_size, max_steps, vocab_size, blocks_amount, heads)\n",
        "encoder_out  = encoder(data)\n",
        "print(\"max_steps {}\\nembedding_size/d_model {}\\nvocab_size {}\\nheads_number {}\\nblocks_amount {}\" .format(max_steps, embed_size, vocab_size, heads, blocks_amount))\n",
        "print(encoder_out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape  (32, 3)\n",
            "WARNING:tensorflow:Layer encoder_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "q shape (32, 3, 10)\n",
            "k shape (32, 3, 10)\n",
            " v shape (32, 3, 10)\n",
            "AFTER Dense\n",
            "  q shape (32, 3, 10)\n",
            "  k shape (32, 3, 10)\n",
            "  v shape (32, 3, 10)\n",
            "AFTER SPLIT\n",
            "  q shape (32, 5, 3, 2)\n",
            "  k shape (32, 5, 3, 2)\n",
            "  v shape (32, 5, 3, 2)\n",
            "tf.Tensor(\n",
            "[[[[0. 1. 1.]\n",
            "   [0. 0. 1.]\n",
            "   [0. 0. 0.]]]], shape=(1, 1, 3, 3), dtype=float64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mnot_equal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   6990\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6991\u001b[0;31m         \"incompatible_shape_error\", incompatible_shape_error)\n\u001b[0m\u001b[1;32m   6992\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-c213c87836a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks_amount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoder_out\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_steps {}\\nembedding_size/d_model {}\\nvocab_size {}\\nheads_number {}\\nblocks_amount {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-a44ff8a2be1e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0;31m# mhatt_output shape [batch_size, max_sentence_len, embedding_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mmhatt_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mhatt_output shape \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmhatt_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-94491f17673f>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, q, k, v, sequence_mask)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AFTER SPLIT\\n  q shape {}\\n  k shape {}\\n  v shape {}\"\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context_vector shape :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-94491f17673f>\u001b[0m in \u001b[0;36mScaledDotProductAttention\u001b[0;34m(self, v, k, q, sequence_mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# we`re adding big negative number, because we only care about present/past words that are przedicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msequence_mask\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" mask is not none\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msequence_mask\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mtensor_not_equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincompatible_shape_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# In legacy graph mode, tensor equality is object equality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mnot_equal\u001b[0;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[1;32m   6995\u001b[0m         return not_equal_eager_fallback(\n\u001b[1;32m   6996\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincompatible_shape_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mincompatible_shape_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6997\u001b[0;31m             name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   6998\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6999\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mnot_equal_eager_fallback\u001b[0;34m(x, y, incompatible_shape_error, name, ctx)\u001b[0m\n\u001b[1;32m   7036\u001b[0m     \u001b[0mincompatible_shape_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7037\u001b[0m   \u001b[0mincompatible_shape_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincompatible_shape_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"incompatible_shape_error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7038\u001b[0;31m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_inputs_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7039\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inputs_T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7040\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, default_dtype)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;31m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minternal_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;31m# TODO(slebedev): consider removing this as it leaks a Keras concept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    284\u001b[0m                                          as_ref=False):\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhbwxVGBVwzP",
        "colab_type": "code",
        "outputId": "0283ecf1-228b-4d95-e68d-22ca2f64103c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "Decoder flow :\n",
        "\n",
        "- Embedding \n",
        "- Positional Encoding\n",
        "- Input = Embedding + Positional Encoding\n",
        "--------------------REPEAT N Times--------------------\n",
        "- Masked Multi-head Attention layer\n",
        "- Input + Masked Multi-Head Attention layer added together \n",
        "- previous Normalized (1) \n",
        "- Multi-head Attention layer v, k from Encoder output | q from previous point\n",
        "- (1) + Multi-head Attention layer added together\n",
        "- previous normalized\n",
        "- Feed Forward Network (2)\n",
        "- (1) added to (2) and Normalized\n",
        "------------------------------------------------------\n",
        "- Decoder output\n",
        "- Linear layer\n",
        "- softmax\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDecoder flow :\\n\\n- Embedding \\n- Positional Encoding\\n- Input = Embedding + Positional Encoding\\n--------------------REPEAT N Times--------------------\\n- Masked Multi-head Attention layer\\n- Input + Masked Multi-Head Attention layer added together \\n- previous Normalized (1) \\n- Multi-head Attention layer v, k from Encoder output | q from previous point\\n- (1) + Multi-head Attention layer added together\\n- previous normalized\\n- Feed Forward Network (2)\\n- (1) added to (2) and Normalized\\n------------------------------------------------------\\n- Decoder output\\n- Linear layer\\n- softmax\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrE1qDbL3i0G",
        "colab_type": "code",
        "outputId": "884f36d5-893d-47b7-bd01-784688161006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=505, shape=(1, 1, 6, 6), dtype=float64, numpy=\n",
              "array([[[[0., 1., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1., 1.],\n",
              "         [0., 0., 0., 1., 1., 1.],\n",
              "         [0., 0., 0., 0., 1., 1.],\n",
              "         [0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0.]]]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwQ_53waKJQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}