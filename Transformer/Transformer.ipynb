{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mizzmir/NLP/blob/master/Transformer/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "160XBLnmv-sP",
        "colab_type": "code",
        "outputId": "8d4f7eee-aed6-4a38-f64a-7ced7c0edf48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.17.3)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.0.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.7)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHsXhZrJv_cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tf.__version__\n",
        "\n",
        "embed_size = 10; max_steps = 3; vocab_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8_4Bkpw11pU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncodingSimpleLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_dim, max_sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingSimpleLayer, self).__init__(dtype=tf.float32, **kwargs)\n",
        "    if embedding_dim %2 != 0:\n",
        "      embedding_dim+=1\n",
        "    PE = np.zeros((1, max_sentence_len, embedding_dim))\n",
        "    for pos in range(max_sentence_len):\n",
        "      for i in range(embedding_dim//2):\n",
        "        PE[:, pos, 2*i] = np.sin(pos/10000**(2*i/embedding_dim))\n",
        "        PE[:, pos, 2*i+1] = np.cos(pos/10000**(2*i/embedding_dim))\n",
        "    tf.print(PE.shape)\n",
        "    self.PE = PE\n",
        "  def call(self, input):\n",
        "    return self.PE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul4lfwi89x6T",
        "colab_type": "code",
        "outputId": "9b4f111d-375e-4532-8539-b45e5b0ebd2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "positional_encoding_layer = PositionalEncodingSimpleLayer(embed_size, max_sentence_len=max_steps)\n",
        "res2 = positional_encoding_layer([1,2,3,4,5])\n",
        "print(res2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "[[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00]\n",
            "  [ 8.41470985e-01  5.40302306e-01  1.57826640e-01  9.87466836e-01\n",
            "    2.51162229e-02  9.99684538e-01  3.98106119e-03  9.99992076e-01\n",
            "    6.30957303e-04  9.99999801e-01]\n",
            "  [ 9.09297427e-01 -4.16146837e-01  3.11697146e-01  9.50181503e-01\n",
            "    5.02165994e-02  9.98738351e-01  7.96205928e-03  9.99968302e-01\n",
            "    1.26191435e-03  9.99999204e-01]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_EB-6u-2Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncodingArangePos(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, max_sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingArangePos, self).__init__(dtype, **kwargs)\n",
        "    if embedding_size%2 !=0:\n",
        "      embedding_size+=1\n",
        "    PE = np.zeros((1, max_sentence_len, embedding_size))\n",
        "    print(PE.shape)\n",
        "    pos = np.arange(start=0, stop=max_sentence_len, step=1)\n",
        "    print(pos.shape)\n",
        "    for i in range(embedding_size//2):\n",
        "      PE[0, ::, 2*i] = np.sin(pos/10000**(2*i/embedding_size))\n",
        "      PE[0, ::, 2*i+1] = np.cos(pos/10000**(2*i/embedding_size))\n",
        "    self.PE = PE\n",
        "  def call(self, inputs):\n",
        "    return self.PE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th4cZj04BZuB",
        "colab_type": "code",
        "outputId": "e70d776e-8942-4fa0-a8ee-d82454254ff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "peLayer = PositionalEncodingArangePos(embed_size, max_sentence_len=max_steps)\n",
        "res3 = peLayer([1,2,3,4,5])\n",
        "print(res3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "(3,)\n",
            "[[[ 0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "    0.00000000e+00  1.00000000e+00]\n",
            "  [ 8.41470985e-01  5.40302306e-01  1.57826640e-01  9.87466836e-01\n",
            "    2.51162229e-02  9.99684538e-01  3.98106119e-03  9.99992076e-01\n",
            "    6.30957303e-04  9.99999801e-01]\n",
            "  [ 9.09297427e-01 -4.16146837e-01  3.11697146e-01  9.50181503e-01\n",
            "    5.02165994e-02  9.98738351e-01  7.96205928e-03  9.99968302e-01\n",
            "    1.26191435e-03  9.99999204e-01]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jeicAkABlgx",
        "colab_type": "code",
        "outputId": "a4f2e43c-b369-4478-d401-9b0a0e7c9f80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"comparing arrays: \", np.allclose(res2, res3))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comparing arrays:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ADeR0AGPsQ",
        "colab_type": "code",
        "outputId": "e8bc33b7-4ebf-4932-b09a-a2709da673fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "Encoder flow :\n",
        "\n",
        "- Embedding \n",
        "- Positional Encoding\n",
        "- Input = Embedding + Positional Encoding\n",
        "--------------------REPEAT N Times--------------------\n",
        "- Multi-head Attention layer\n",
        "- Input + Multi-Head Attention layer added together \n",
        "- previous Normalized (1)\n",
        "- Feed Forward Network (2)\n",
        "- (1) added to (2) and Normmalized\n",
        "------------------------------------------------------\n",
        "- Encoder output \n",
        "\"\"\""
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEncoder flow :\\n\\n- Embedding \\n- Positional Encoding\\n- Input = Embedding + Positional Encoding\\n--------------------REPEAT N Times--------------------\\n- Multi-head Attention layer\\n- Input + Multi-Head Attention layer added together \\n- previous Normalized (1)\\n- Feed Forward Network (2)\\n- (1) added to (2) and Normmalized\\n------------------------------------------------------\\n- Encoder output \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI8KlrtP_ay8",
        "colab_type": "code",
        "outputId": "9699c9da-4f24-4ccb-db82-0e4b351707d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, max_sentence_len, dtype=tf.float32, **kwargs):\n",
        "    super(PositionalEncodingLayer, self).__init__(dtype, **kwargs)\n",
        "    if embedding_size%2 !=0:\n",
        "      embedding_size+=1\n",
        "    # embedding size -> depth of model\n",
        "    # positional encoding should have size : [1, max_sentence_len, embedding_size]\n",
        "    # 1 is here to make broadcasting possible in call method\n",
        "    PE = np.zeros((1, max_sentence_len, embedding_size))\n",
        "    # pos should have shape [1, max_sentence_len] with values <0, max_sentence_len)\n",
        "    pos = np.arange(start=0, stop=max_sentence_len, step=1)\n",
        "    pos = pos.reshape(max_sentence_len, 1)\n",
        "    # i should have shappe [1, embedding_size//2] with values <0, embedding_size//2)\n",
        "    # we need half of embedding size, because half is needed for each sin/cos \n",
        "    # then we put it together into PE and we have [1, max_sentence_len, embedding_size]\n",
        "    i = np.arange(start=0, stop=embedding_size//2, step=1)\n",
        "    i = i.reshape(embedding_size//2, 1).T\n",
        "    PE_sin = np.sin(pos/10000**(2*i/embedding_size))\n",
        "    PE_cos = np.cos(pos/10000**(2*i/embedding_size))\n",
        "    # we put sin into even indexes ::2 \n",
        "    # we put cos into odd indexes, thats why we`re starting from 1 here : 1::2\n",
        "    PE[0, ::, ::2] = PE_sin\n",
        "    PE[0, ::, 1::2] = PE_cos\n",
        "    self.PE = tf.constant(PE, dtype=dtype)\n",
        "  def getPE(self):\n",
        "    \"\"\"\n",
        "    only for debuging purposes\n",
        "    \"\"\"\n",
        "    return self.PE\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "    inputs shape should be same as self.PE shape\n",
        "        \n",
        "      input_shape = tf.shape(inputs)\n",
        "      return inputs + self.PE[:, :input_shape[-2], :]\n",
        "\n",
        "    It has to be that way becuase we need to be able to get positional encoding for different lenght \n",
        "    for encoder and decoder, when we don`t know max lenght. SO we have to do encoding with bigger buffer\n",
        "    and take what we need only.\n",
        "\n",
        "    max_sentence_len in should be bigger or equal as longest input we predict we can get\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = tf.shape(inputs)\n",
        "    return inputs + self.PE[:, :input_shape[-2], :]\n",
        "\n",
        "peLayerAll = PositionalEncodingLayer(embedding_size=10,\n",
        "                                     max_sentence_len=max_steps)\n",
        "res4 = peLayerAll.getPE()\n",
        "print(res4.shape)\n",
        "print(\"comparing arrays: \", np.allclose(res2, res4))"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 10)\n",
            "comparing arrays:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqxOYEMvhs4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, heads_number, dtype=tf.float32, **kwargs):\n",
        "    super(MultiHeadAttentionLayer, self).__init__(dtype=tf.float32, **kwargs)\n",
        "    \"\"\"\n",
        "    return shape : [batch_size, sequence_len, d_model]\n",
        "    heads_number - tell how many heads will be processed at same time\n",
        "    d_model - model size ; equal to embedding_size\n",
        "    \"\"\"\n",
        "    self.heads_number = heads_number\n",
        "    self.d_model = embedding_size\n",
        "    self.w_q = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_k = tf.keras.layers.Dense(self.d_model)\n",
        "    self.w_v = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    self.outputLayer = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  # similar to dot attention but with scaling added\n",
        "  def ScaledDotProductAttention(self, v, k, q, sequence_mask):\n",
        "    \"\"\"\n",
        "    q shape [batch_size, num_heads, q_seq_len, depth_q]\n",
        "    k shape [batch_size, num_heads, k_seq_len, depth_k]\n",
        "    v shape [batch_size, num_heads, v_seq_len, depth_v]\n",
        "    \"\"\"\n",
        "    # matmul(q,k,v)\n",
        "    # resultion shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    qk_matmul = tf.matmul(q, k, transpose_b=True)\n",
        "    # scaling tf.cast is needed here because tf.sqrt needs float32 type\n",
        "    # score shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    score = qk_matmul*tf.math.sqrt(tf.cast(k.shape[-1], dtype=tf.float32))\n",
        "    # optional mask\n",
        "    # mask should be shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    # for example [\n",
        "    #             [0, 1, 1]\n",
        "    #             [0, 0, 1]\n",
        "    #             ] shape == (2, 3)\n",
        "    # we`re adding big negative number, because we only care about present/past words that are przedicted\n",
        "    if sequence_mask is not None:\n",
        "      #print(\" mask is not none\")\n",
        "      #print(\"sequence_mask shape {}\\nscore shape {}\" .format(sequence_mask.shape, score.shape))\n",
        "      score += sequence_mask*-1e-8\n",
        "    # softmax\n",
        "    # attention_weights shape [batch_size, num_heads, q_seq_len, k_seq_len]\n",
        "    attention_weights = tf.nn.softmax(score, axis=-1)\n",
        "    # matmul(res, V)\n",
        "    # contex shape [batch_size, num_heads, q_seq_len, depth_v]\n",
        "    context = tf.matmul(attention_weights, v)\n",
        "    return context\n",
        "\n",
        "  def splitHeads(self, data):\n",
        "    # new shape [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "    data = tf.reshape(data, (data.shape[0], data.shape[1], self.heads_number, data.shape[-1]//self.heads_number))\n",
        "    # transpose dimentions to [batch_size, heads_number, sequence_len, d_model//heads_number]\n",
        "    return tf.transpose(data, perm=[0,2,1,3])\n",
        "\n",
        "  def call(self, q, k, v, sequence_mask):\n",
        "    \"\"\"\n",
        "    q shape [batch_size, sequence_len, d_model]\n",
        "    k shape [batch_size, sequence_len, d_model]\n",
        "    v shape [batch_size, sequence_len, d_model]\n",
        "\n",
        "    after first operations shapes are the same\n",
        "    next we have to split d_model into heads_number of subbatches\n",
        "    new shape after reshape only should be : [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "    next shape should be transposed to : [batch_size, heads_number, sequence_len, d_model//heads_number]\n",
        "    where :\n",
        "      new_d_model = d_model/heads_number\n",
        "    \n",
        "    next make scaled dot-product attention on resulting q,k,v\n",
        "\n",
        "    next concat returning data to get shape : [batch_size, sequence_len, d_model]\n",
        "    in order to do this we have to transpose context_vector to get [batch_size, sequence_len, heads_number, d_model//heads_number]\n",
        "\n",
        "    next put it throug dense layer (d_model) in order to get output\n",
        "    \"\"\"\n",
        "    #print(\"q shape {}\\nk shape {}\\n v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "    q = self.w_q(q)\n",
        "    k = self.w_k(k)\n",
        "    v = self.w_v(v)\n",
        "    #print(\"AFTER Dense\\n  q shape {}\\n  k shape {}\\n  v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "\n",
        "    q = self.splitHeads(q)\n",
        "    k = self.splitHeads(k)\n",
        "    v = self.splitHeads(v)\n",
        "    #print(\"AFTER SPLIT\\n  q shape {}\\n  k shape {}\\n  v shape {}\" .format(q.shape, k.shape, v.shape))\n",
        "\n",
        "    context_vector = self.ScaledDotProductAttention(q, k, v, sequence_mask)\n",
        "    #print(\"context_vector shape :\", context_vector.shape)\n",
        "\n",
        "    context_vector = tf.transpose(context_vector, perm=[0,2,1,3])\n",
        "    #print(\"context_vector  transposed shape :\", context_vector.shape)\n",
        "    context_vector = tf.reshape(context_vector, (context_vector.shape[0], context_vector.shape[1], self.d_model))\n",
        "    #print(\"context_vector  reshapeed shape :\", context_vector.shape)\n",
        "\n",
        "    return self.outputLayer(context_vector)\n",
        "\n",
        "embed_size = 10; max_steps = 3; vocab_size = 100\n",
        "\n",
        "q = tf.random.uniform((1, max_steps, embed_size))  # shape [batch_size, sequence_len, embedding_size]\n",
        "mhatt = MultiHeadAttentionLayer(embed_size, 5)\n",
        "mhatt_output = mhatt(q, k=q, v=q, sequence_mask=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4GNTftVHb5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feedForwardnetwork(dff, d_model):\n",
        "  \"\"\"\n",
        "  according to paper dff=2048 and d_model =512\n",
        "  but d_model should be same as embedding_size/d_model in MultiHeadAttention\n",
        "  ffn(x) = max(0, xW_1 + b+1)W_2 + b_2\n",
        "  where max(0, ...) -> relu activation\n",
        "  \"\"\"\n",
        "  ffNetwork = tf.keras.Sequential()\n",
        "  ffNetwork.add(tf.keras.layers.Dense(dff, activation=\"relu\"))\n",
        "  ffNetwork.add(tf.keras.layers.Dense(d_model))\n",
        "  return ffNetwork\n",
        "\n",
        "def makeSequenceMask(seq_len):\n",
        "  \"\"\"\n",
        "  mask should be size [1, 1, seq_len, seq_len]\n",
        "  first two sizes are batch_szie, num_heads to make this matrix broadcastable\n",
        "  it should be in form \n",
        "  [\n",
        "    [0, 1, 1, 1]\n",
        "    [0, 0, 1, 1]\n",
        "    [0, 0, 0, 1]\n",
        "    [0, 0, 0, 0]\n",
        "  ]\n",
        "  \"\"\"\n",
        "  mask_array = np.ones((seq_len, seq_len))\n",
        "  mask_array = np.triu(mask_array, 1)\n",
        "  return tf.constant(mask_array, dtype=tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-_p_e0UNCpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, heads_number, dff, dtype=tf.float32, **kwargs):\n",
        "    super(EncoderLayer, self).__init__(dtype, **kwargs)\n",
        "\n",
        "    self.d_model = embedding_size\n",
        "    self.multiHeadAttention = MultiHeadAttentionLayer(embedding_size, heads_number)\n",
        "\n",
        "    self.normalizationFirst = tf.keras.layers.LayerNormalization()\n",
        "    self.normalizationSecond = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    self.ffNetwork = feedForwardnetwork(dff, self.d_model)\n",
        "\n",
        "  def call(self, encoder_input, mask):\n",
        "    # shortcut_data shape [batch_szie, max_sentence_len, embedding_size]\n",
        "    shortcut_data = encoder_input\n",
        "\n",
        "    # mhatt_output shape [batch_size, max_sentence_len, embedding_size]\n",
        "    mhatt_output = self.multiHeadAttention(encoder_input, encoder_input, encoder_input, mask)\n",
        "    mhatt_output += shortcut_data\n",
        "    mhatt_output = self.normalizationFirst(mhatt_output)\n",
        "\n",
        "    shortcut_data = mhatt_output\n",
        "\n",
        "    ffNet_output = self.ffNetwork(mhatt_output)\n",
        "    ffNet_output += shortcut_data\n",
        "    ffNet_output = self.normalizationSecond(ffNet_output)\n",
        "\n",
        "    return ffNet_output\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_size, max_sentence_len, vocab_size, blocks_amount, heads_number, dff):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    assert (embedding_size//heads_number)%2==0\n",
        "    self.blocks_amount = blocks_amount\n",
        "    self.d_model = embedding_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.positionalEncoding = PositionalEncodingLayer(embedding_size, max_sentence_len)\n",
        "\n",
        "    self.encoderBlocks = [EncoderLayer(embedding_size, heads_number, dff) for _ in range(blocks_amount)]\n",
        "  \n",
        "  def call(self, encoder_input, mask):\n",
        "    # sequence shape [batch_size, max_sentence_len]\n",
        "    embedded_seq = self.embedding(encoder_input)\n",
        "    # according to paper https://arxiv.org/pdf/1706.03762.pdf\n",
        "    # embedding is multiplied by sqrt(d_model). Point 3.4\n",
        "    embedded_seq*=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    # embedded_seq shape [batch_szie, max_sentence_len, embedding_size]\n",
        "    data = self.positionalEncoding(embedded_seq)\n",
        "    #------------------------- loop though all blocks -------------------------\n",
        "    for i in range(self.blocks_amount):\n",
        "      print(\"               BLOCK \", i+1)\n",
        "      data = self.encoderBlocks[i](data, mask) \n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T85ncKrVRYC",
        "colab_type": "code",
        "outputId": "d2c0c17f-ca46-4264-f836-a85729d0e316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "data = np.ones((32, max_steps))\n",
        "print(\"input shape \", data.shape)\n",
        "\n",
        "encoder = Encoder(embedding_size=10,\n",
        "                  max_sentence_len=1000,\n",
        "                  vocab_size=100,\n",
        "                  blocks_amount=3,\n",
        "                  heads_number=5, \n",
        "                  dff=2048)\n",
        "encoder_out  = encoder(data, mask=None)\n",
        "print(\"max_steps {}\\nembedding_size/d_model {}\\nvocab_size {}\\nheads_number {}\\nblocks_amount {}\" .format(max_steps, embed_size, vocab_size, heads, blocks_amount))\n",
        "print(encoder_out.shape)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input shape  (32, 3)\n",
            "WARNING:tensorflow:Layer encoder_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "               BLOCK  1\n",
            "               BLOCK  2\n",
            "               BLOCK  3\n",
            "max_steps 3\n",
            "embedding_size/d_model 10\n",
            "vocab_size 100\n",
            "heads_number 5\n",
            "blocks_amount 2\n",
            "(32, 3, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhbwxVGBVwzP",
        "colab_type": "code",
        "outputId": "fc7a0c4e-b867-49c6-ac54-633727dc0163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "\"\"\"\n",
        "Decoder flow :\n",
        "\n",
        "- Embedding \n",
        "- Positional Encoding\n",
        "- Input = Embedding + Positional Encoding\n",
        "--------------------REPEAT N Times--------------------\n",
        "- Masked Multi-head Attention layer\n",
        "- Input + Masked Multi-Head Attention layer added together \n",
        "- previous Normalized (1) \n",
        "- Multi-head Attention layer v, k from Encoder output | q from previous point\n",
        "- (1) + Multi-head Attention layer added together\n",
        "- previous normalized\n",
        "- Feed Forward Network (2)\n",
        "- (1) added to (2) and Normalized\n",
        "------------------------------------------------------\n",
        "- Decoder output\n",
        "- Linear layer\n",
        "- softmax\n",
        "\"\"\""
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDecoder flow :\\n\\n- Embedding \\n- Positional Encoding\\n- Input = Embedding + Positional Encoding\\n--------------------REPEAT N Times--------------------\\n- Masked Multi-head Attention layer\\n- Input + Masked Multi-Head Attention layer added together \\n- previous Normalized (1) \\n- Multi-head Attention layer v, k from Encoder output | q from previous point\\n- (1) + Multi-head Attention layer added together\\n- previous normalized\\n- Feed Forward Network (2)\\n- (1) added to (2) and Normalized\\n------------------------------------------------------\\n- Decoder output\\n- Linear layer\\n- softmax\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrE1qDbL3i0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, embedding_size, heads_number, dff, dtype=tf.float32, **kwargs):\n",
        "    super(DecoderLayer, self).__init__(dtype, **kwargs)\n",
        "\n",
        "    self.d_model = embedding_size\n",
        "    self.multiHeadAttentionFirst = MultiHeadAttentionLayer(embedding_size, heads_number)\n",
        "    self.multiHeadAttentionSecond = MultiHeadAttentionLayer(embedding_size, heads_number)\n",
        "\n",
        "    self.normalizationFirst = tf.keras.layers.LayerNormalization()\n",
        "    self.normalizationSecond = tf.keras.layers.LayerNormalization()\n",
        "    self.normalizationThird = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    self.ffNetwork = feedForwardnetwork(dff, self.d_model)\n",
        "\n",
        "  def call(self, decoder_input, encoder_output, mask):\n",
        "    # shortcut_data shape [batch_szie, max_sentence_len, embedding_size]\n",
        "    shortcut_data = decoder_input\n",
        "      \n",
        "    # mhatt_output shape [batch_size, max_sentence_len, embedding_size]\n",
        "    mhatt_output = self.multiHeadAttentionFirst(decoder_input, decoder_input, decoder_input, mask)\n",
        "    # add & Norm\n",
        "    mhatt_output += shortcut_data\n",
        "    mhatt_output = self.normalizationFirst(mhatt_output)\n",
        "\n",
        "    shortcut_data = mhatt_output\n",
        "    mhatt_output = self.multiHeadAttentionSecond(encoder_output, encoder_output, mhatt_output, None)\n",
        "    mhatt_output += shortcut_data\n",
        "    mhatt_output = self.normalizationSecond(mhatt_output)\n",
        "\n",
        "    shortcut_data = mhatt_output\n",
        "    ffn_output = self.ffNetwork(mhatt_output)\n",
        "    ffn_output += shortcut_data\n",
        "    ffNet_output = self.normalizationThird(ffn_output)\n",
        "\n",
        "    return ffNet_output\n",
        "\n",
        "class Decoder(tf.keras.models.Model):\n",
        "  def __init__(self, embedding_size, max_sentence_len, vocab_size, blocks_amount, heads_number, dff):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    assert (embedding_size//heads_number)%2==0\n",
        "    self.blocks_amount = blocks_amount\n",
        "    self.d_model = embedding_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "    self.positionalEncoding = PositionalEncodingLayer(embedding_size, max_sentence_len)\n",
        "\n",
        "    self.decoderBlocks = [DecoderLayer(embedding_size, heads_number, dff) for _ in range(blocks_amount)]\n",
        "\n",
        "  def call(self, encoder_output, decoder_input, mask):\n",
        "\n",
        "    # sequence shape [batch_size, max_sentence_len]\n",
        "    embedded_seq = self.embedding(decoder_input)\n",
        "    # according to paper https://arxiv.org/pdf/1706.03762.pdf\n",
        "    # embedding is multiplied by sqrt(d_model). Point 3.4\n",
        "    embedded_seq*=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    # embedded_seq shape [batch_szie, max_sentence_len, embedding_size]\n",
        "    data = self.positionalEncoding(embedded_seq)\n",
        "    #------------------------- loop though all blocks -------------------------\n",
        "    for i in range(self.blocks_amount):\n",
        "      print(\"               BLOCK \", i+1)\n",
        "      data = self.decoderBlocks[i](data, encoder_output, mask)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8Wb0q1bvlxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "deb128de-fecd-4fa3-a8b2-9cde998b71a7"
      },
      "source": [
        "input_data = np.ones((32, max_steps))\n",
        "output_data = tf.random.uniform((32, 15))\n",
        "mask = makeSequenceMask(max_steps)\n",
        "print(\"Decoder input shape \", data.shape)\n",
        "blocks_amount = 2\n",
        "heads = 5\n",
        "en_vocab_size = 100\n",
        "fr_vocab_size = 200\n",
        "decoder = Decoder(embedding_size=10,\n",
        "                  max_sentence_len=1000,\n",
        "                  vocab_size=100,\n",
        "                  blocks_amount=3,\n",
        "                  heads_number=5, \n",
        "                  dff=2048)\n",
        "decoder_out  = decoder(encoder_out, data, mask=mask)\n",
        "print(\"decoder_out \", decoder_out.shape)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder input shape  (32, 3)\n",
            "               BLOCK  1\n",
            "               BLOCK  2\n",
            "               BLOCK  3\n",
            "decoder_out  (32, 3, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM03dp3qwO_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.models.Model):\n",
        "  def __init__(self,\n",
        "               embedding_size,\n",
        "               dff,\n",
        "               input_max_seq_length,\n",
        "               output_max_seq_length,\n",
        "               input_vocab_size,\n",
        "               output_vocab_size,\n",
        "               encoder_blocks,\n",
        "               decoder_blocks,\n",
        "               heads):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(embedding_size, input_max_seq_length, input_vocab_size, encoder_blocks, heads, dff)\n",
        "    self.decoder = Decoder(embedding_size, output_max_seq_length, output_vocab_size, decoder_blocks, heads, dff)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(output_vocab_size)\n",
        "\n",
        "  def call(self, input_seq, output_seq, words_mask):\n",
        "    \n",
        "    encoder_out = self.encoder(input_seq, mask=None)\n",
        "    print(encoder_out.shape)\n",
        "    decoder_out = self.decoder(encoder_out, output_seq, mask=words_mask)\n",
        "\n",
        "    # transformer_out shape = [batch_size, ]\n",
        "    transformer_out = self.dense(decoder_out)\n",
        "    return transformer_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrJPalvR37Xp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "d6c13b20-9e4c-404a-bbf2-c5835d92b340"
      },
      "source": [
        "transformer_model = Transformer(embedding_size=512,\n",
        "                                dff=2048,\n",
        "                                input_max_seq_length=2000,\n",
        "                                output_max_seq_length=1855,\n",
        "                                input_vocab_size=4980,\n",
        "                                output_vocab_size=7001,\n",
        "                                encoder_blocks=4,\n",
        "                                decoder_blocks=2,\n",
        "                                heads=8)\n",
        "\n",
        "# input_data and output_data\n",
        "input_data = tf.random.uniform((64, 52), dtype=tf.int64, minval=0, maxval=100)\n",
        "output_data = tf.random.uniform((64, 29), dtype=tf.int64, minval=0, maxval=250)\n",
        "\n",
        "\n",
        "transformer_output = transformer_model(input_data, output_data, words_mask=None)\n",
        "print(transformer_output.shape)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               BLOCK  1\n",
            "               BLOCK  2\n",
            "               BLOCK  3\n",
            "               BLOCK  4\n",
            "(64, 52, 512)\n",
            "               BLOCK  1\n",
            "               BLOCK  2\n",
            "(64, 29, 7001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6QCBpOj4OAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}